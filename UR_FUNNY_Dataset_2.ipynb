{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "VcnJ-i8r0_gu",
        "outputId": "6876544f-4a40-4e2b-9ec4-40f1f1ac534b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cuda\n",
            "\n",
            "Batch size: 4\n",
            "Epochs: 15\n",
            "Data folder: ./ted_humor_data\n",
            "\n",
            "Loading data\n",
            "Train: 10598, Dev: 2626, Test: 3290\n",
            "\n",
            "======================================================================\n",
            "PHASE 1: BERT + LSTM AUDIO/VIDEO (Temporal Sequence Modeling)\n",
            "======================================================================\n",
            "Epoch  3 | Loss: 0.6884 | Dev Acc: 0.5967 | F1: 0.5938\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3625377951.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3625377951.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0mbest_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0mdev_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3625377951.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         logits = model(word_indices, audio, video,\n\u001b[0m\u001b[1;32m    371\u001b[0m                       audio_lengths, video_lengths, attention_mask)\n\u001b[1;32m    372\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3625377951.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_indices, audio, video, audio_lengths, video_lengths, attention_mask)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# Audio encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0maudio_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0maudio_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1769\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1771\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from transformers import BertModel\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using: {DEVICE}\\n\")\n",
        "\n",
        "# Settings\n",
        "BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 15\n",
        "LEARNING_RATE = 2e-5\n",
        "DATA_FOLDER = './ted_humor_data'\n",
        "SAVE_FOLDER = './saved_models'\n",
        "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
        "\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Data folder: {DATA_FOLDER}\\n\")\n",
        "\n",
        "# Dataset Class\n",
        "\n",
        "class BertLSTMDataset(Dataset):\n",
        "    \"\"\"Dataset that preserves temporal sequences for Audio and Video.\"\"\"\n",
        "\n",
        "    def __init__(self, ids, text_data, audio_data, video_data, labels,\n",
        "                 word_embeddings, audio_scaler, video_scaler, is_train=False):\n",
        "        self.ids = ids\n",
        "        self.text_data = text_data\n",
        "        self.audio_data = audio_data\n",
        "        self.video_data = video_data\n",
        "        self.labels = labels\n",
        "        self.word_embeddings = word_embeddings\n",
        "        self.audio_scaler = audio_scaler\n",
        "        self.video_scaler = video_scaler\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_id = self.ids[idx]\n",
        "\n",
        "        # Text\n",
        "        text_data = self.text_data[sample_id]\n",
        "        if isinstance(text_data, dict):\n",
        "            if 'punchline_features' in text_data:\n",
        "                text_indices = text_data['punchline_features']\n",
        "            elif 'punchline' in text_data:\n",
        "                text_indices = text_data['punchline']\n",
        "            else:\n",
        "                text_indices = list(text_data.values())[0]\n",
        "        else:\n",
        "            text_indices = text_data\n",
        "\n",
        "        word_indices = []\n",
        "        if isinstance(text_indices, (list, np.ndarray)):\n",
        "            for idx_list in text_indices:\n",
        "                try:\n",
        "                    if isinstance(idx_list, (list, np.ndarray)) and len(idx_list) > 0:\n",
        "                        idx_val = int(idx_list[0]) if hasattr(idx_list[0], '__int__') else 0\n",
        "                    else:\n",
        "                        idx_val = int(idx_list) if hasattr(idx_list, '__int__') else 0\n",
        "\n",
        "                    if 0 <= idx_val < 30522:  # BERT vocab size\n",
        "                        word_indices.append(idx_val)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        if len(word_indices) == 0:\n",
        "            word_indices = [101]  # [CLS] token\n",
        "        word_indices = word_indices[:512]\n",
        "\n",
        "        # Audio\n",
        "        # KEEP as sequence: shape (T, 81)\n",
        "        audio_data = self.audio_data[sample_id]\n",
        "        if isinstance(audio_data, dict):\n",
        "            if 'punchline_features' in audio_data:\n",
        "                audio_features = audio_data['punchline_features']\n",
        "            elif 'punchline' in audio_data:\n",
        "                audio_features = audio_data['punchline']\n",
        "            else:\n",
        "                audio_features = list(audio_data.values())[0]\n",
        "        else:\n",
        "            audio_features = audio_data\n",
        "\n",
        "        try:\n",
        "            audio_seq = np.array(audio_features, dtype=np.float32).reshape(-1, 81)\n",
        "            if audio_seq.shape[0] == 0:\n",
        "                audio_seq = np.zeros((1, 81), dtype=np.float32)\n",
        "\n",
        "            if self.audio_scaler:\n",
        "                audio_seq = self.audio_scaler.transform(audio_seq)\n",
        "        except:\n",
        "            audio_seq = np.zeros((1, 81), dtype=np.float32)\n",
        "\n",
        "        # Video\n",
        "        # KEEP as sequence: shape (T, 75)\n",
        "        video_data = self.video_data[sample_id]\n",
        "        if isinstance(video_data, dict):\n",
        "            if 'punchline_features' in video_data:\n",
        "                video_features = video_data['punchline_features']\n",
        "            elif 'punchline' in video_data:\n",
        "                video_features = video_data['punchline']\n",
        "            else:\n",
        "                video_features = list(video_data.values())[0]\n",
        "        else:\n",
        "            video_features = video_data\n",
        "\n",
        "        try:\n",
        "            video_seq = np.array(video_features, dtype=np.float32).reshape(-1, 75)\n",
        "            if video_seq.shape[0] == 0:\n",
        "                video_seq = np.zeros((1, 75), dtype=np.float32)\n",
        "\n",
        "            if self.video_scaler:\n",
        "                video_seq = self.video_scaler.transform(video_seq)\n",
        "        except:\n",
        "            video_seq = np.zeros((1, 75), dtype=np.float32)\n",
        "\n",
        "        #  Modality Dropout\n",
        "        if self.is_train and np.random.rand() < 0.2:\n",
        "            which = np.random.choice(['text', 'audio', 'video'])\n",
        "            if which == 'text':\n",
        "                word_indices = [101]\n",
        "            elif which == 'audio':\n",
        "                audio_seq = np.zeros((1, 81), dtype=np.float32)\n",
        "            elif which == 'video':\n",
        "                video_seq = np.zeros((1, 75), dtype=np.float32)\n",
        "\n",
        "        label = self.labels[sample_id]\n",
        "\n",
        "        return {\n",
        "            'word_indices': torch.LongTensor(word_indices),\n",
        "            'audio': torch.FloatTensor(audio_seq),  # [T, 81]\n",
        "            'video': torch.FloatTensor(video_seq),  # [T, 75]\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "# Collate Funciton: Handles Variable-Length Sequences\n",
        "\n",
        "\n",
        "def collate_fn_lstm(batch):\n",
        "    \"\"\"Collate function that pads sequences and tracks their original lengths.\"\"\"\n",
        "\n",
        "    # Text\n",
        "    max_text_len = max(len(item['word_indices']) for item in batch)\n",
        "    word_indices_padded = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for item in batch:\n",
        "        indices = item['word_indices'].numpy()\n",
        "        padded = np.pad(indices, (0, max_text_len - len(indices)), constant_values=0)\n",
        "        word_indices_padded.append(torch.LongTensor(padded))\n",
        "\n",
        "        mask = np.ones(len(indices))\n",
        "        mask_padded = np.pad(mask, (0, max_text_len - len(indices)), constant_values=0)\n",
        "        attention_masks.append(torch.LongTensor(mask_padded))\n",
        "\n",
        "    # Audion Sequences\n",
        "    audio_seqs = [item['audio'] for item in batch]  # List of [T_i, 81]\n",
        "    audio_lengths = torch.tensor([len(seq) for seq in audio_seqs])\n",
        "    audio_padded = pad_sequence(audio_seqs, batch_first=True)  # [batch, max_T, 81]\n",
        "\n",
        "    # Video Sequences\n",
        "    video_seqs = [item['video'] for item in batch]  # List of [T_i, 75]\n",
        "    video_lengths = torch.tensor([len(seq) for seq in video_seqs])\n",
        "    video_padded = pad_sequence(video_seqs, batch_first=True)  # [batch, max_T, 75]\n",
        "\n",
        "    return {\n",
        "        'word_indices': torch.stack(word_indices_padded),\n",
        "        'attention_mask': torch.stack(attention_masks),\n",
        "        'audio': audio_padded,\n",
        "        'audio_lengths': audio_lengths,\n",
        "        'video': video_padded,\n",
        "        'video_lengths': video_lengths,\n",
        "        'label': torch.stack([item['label'] for item in batch])\n",
        "    }\n",
        "\n",
        "# Encoder\n",
        "\n",
        "class BertTextEncoder(nn.Module):\n",
        "    \"\"\"BERT-based text encoder (frozen).\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.bert_dim = 768\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        if input_ids.shape[0] == 0 or input_ids.shape[1] == 0:\n",
        "            return torch.zeros(input_ids.shape[0] if input_ids.shape[0] > 0 else 1,\n",
        "                              self.bert_dim).to(input_ids.device)\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "            cls_token = outputs.last_hidden_state[:, 0, :]  # [batch, 768]\n",
        "\n",
        "        return cls_token\n",
        "\n",
        "\n",
        "class LSTMAudioEncoder(nn.Module):\n",
        "    \"\"\"LSTM-based audio encoder that preserves temporal sequences.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=81, hidden_dim=64, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.2 if num_layers > 1 else 0\n",
        "        )\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, audio_seq, lengths):\n",
        "\n",
        "        if audio_seq.shape[0] == 0:\n",
        "            return torch.zeros(1, self.hidden_dim).to(audio_seq.device)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        packed = pack_padded_sequence(audio_seq, lengths.cpu(),\n",
        "                                     batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # LSTM forward\n",
        "        _, (hidden, cell) = self.lstm(packed)\n",
        "\n",
        "\n",
        "        last_hidden = hidden[-1]\n",
        "\n",
        "        return last_hidden\n",
        "\n",
        "\n",
        "class LSTMVideoEncoder(nn.Module):\n",
        "    \"\"\"LSTM-based video encoder that preserves temporal sequences.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=75, hidden_dim=64, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.2 if num_layers > 1 else 0\n",
        "        )\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, video_seq, lengths):\n",
        "\n",
        "        if video_seq.shape[0] == 0:\n",
        "            return torch.zeros(1, self.hidden_dim).to(video_seq.device)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        packed = pack_padded_sequence(video_seq, lengths.cpu(),\n",
        "                                     batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # LSTM forward\n",
        "        _, (hidden, cell) = self.lstm(packed)\n",
        "\n",
        "\n",
        "        last_hidden = hidden[-1]\n",
        "\n",
        "        return last_hidden\n",
        "\n",
        "\n",
        "# Multimodal Model\n",
        "\n",
        "class BertLSTMModel(nn.Module):\n",
        "    \"\"\"Multimodal model with LSTM sequence encoders for audio and video.\"\"\"\n",
        "\n",
        "    def __init__(self, audio_hidden=64, video_hidden=64, lstm_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Text encoder\n",
        "        self.bert_encoder = BertTextEncoder()\n",
        "        text_dim = 768\n",
        "\n",
        "        # Audio encoder\n",
        "        self.audio_encoder = LSTMAudioEncoder(input_dim=81, hidden_dim=audio_hidden,\n",
        "                                              num_layers=lstm_layers)\n",
        "        audio_dim = audio_hidden\n",
        "\n",
        "        # Video encoder\n",
        "        self.video_encoder = LSTMVideoEncoder(input_dim=75, hidden_dim=video_hidden,\n",
        "                                              num_layers=lstm_layers)\n",
        "        video_dim = video_hidden\n",
        "\n",
        "        # Fusion layer\n",
        "        total_dim = text_dim + audio_dim + video_dim\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(total_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, word_indices, audio, video,\n",
        "               audio_lengths, video_lengths, attention_mask=None):\n",
        "        # Text encoding\n",
        "        if word_indices.shape[1] > 0:\n",
        "            text_out = self.bert_encoder(word_indices, attention_mask)\n",
        "        else:\n",
        "            text_out = torch.zeros(audio.shape[0], 768).to(audio.device)\n",
        "\n",
        "        # Audio encoding\n",
        "        if audio.shape[0] > 0 and audio.shape[1] > 0:\n",
        "            audio_out = self.audio_encoder(audio, audio_lengths)\n",
        "        else:\n",
        "            audio_out = torch.zeros(audio.shape[0], 64).to(audio.device)\n",
        "\n",
        "        # Video encoding\n",
        "        if video.shape[0] > 0 and video.shape[1] > 0:\n",
        "            video_out = self.video_encoder(video, video_lengths)\n",
        "        else:\n",
        "            video_out = torch.zeros(video.shape[0], 64).to(video.device)\n",
        "\n",
        "        # Fusion\n",
        "        combined = torch.cat([text_out, audio_out, video_out], dim=1)\n",
        "        fused = self.fusion(combined)\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(fused)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Training Function\n",
        "\n",
        "def train_epoch(model, loader, optimizer, loss_fn, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        word_indices = batch['word_indices'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        audio = batch['audio'].to(device)\n",
        "        audio_lengths = batch['audio_lengths'].to(device)\n",
        "        video = batch['video'].to(device)\n",
        "        video_lengths = batch['video_lengths'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(word_indices, audio, video,\n",
        "                      audio_lengths, video_lengths, attention_mask)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "\n",
        "def evaluate(model, loader, device, missing_mod=None):\n",
        "    \"\"\"Evaluate model with optional missing modality\"\"\"\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            word_indices = batch['word_indices'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            audio = batch['audio'].to(device)\n",
        "            audio_lengths = batch['audio_lengths'].to(device)\n",
        "            video = batch['video'].to(device)\n",
        "            video_lengths = batch['video_lengths'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            # Simulate missing modality\n",
        "            if missing_mod == 'audio':\n",
        "                audio = torch.zeros_like(audio)\n",
        "            elif missing_mod == 'video':\n",
        "                video = torch.zeros_like(video)\n",
        "            elif missing_mod == 'text':\n",
        "                word_indices = torch.zeros_like(word_indices)\n",
        "                attention_mask = torch.zeros_like(attention_mask)\n",
        "\n",
        "            logits = model(word_indices, audio, video,\n",
        "                          audio_lengths, video_lengths, attention_mask)\n",
        "            pred = logits.argmax(dim=1)\n",
        "\n",
        "            preds.extend(pred.cpu().numpy())\n",
        "            targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(targets, preds)\n",
        "    f1 = f1_score(targets, preds, average='macro', zero_division=0)\n",
        "    return acc, f1\n",
        "\n",
        "\n",
        "# Data Loading\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Load and prepare data.\"\"\"\n",
        "    print(\"Loading data\")\n",
        "\n",
        "    with open(f'{DATA_FOLDER}/data_folds.pkl', 'rb') as f:\n",
        "        data_folds = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    with open(f'{DATA_FOLDER}/word_embedding_list.pkl', 'rb') as f:\n",
        "        word_embeddings = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    with open(f'{DATA_FOLDER}/word_embedding_indexes_sdk.pkl', 'rb') as f:\n",
        "        text_features = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    with open(f'{DATA_FOLDER}/covarep_features_sdk.pkl', 'rb') as f:\n",
        "        audio_features = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    with open(f'{DATA_FOLDER}/openface_features_sdk.pkl', 'rb') as f:\n",
        "        video_features = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    with open(f'{DATA_FOLDER}/humor_label_sdk.pkl', 'rb') as f:\n",
        "        labels = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    # Parse folds\n",
        "    train_ids = None\n",
        "    try:\n",
        "        if isinstance(data_folds, dict) and 'train' in data_folds:\n",
        "            train_ids = data_folds['train']\n",
        "            dev_ids = data_folds['dev']\n",
        "            test_ids = data_folds['test']\n",
        "        elif isinstance(data_folds, list):\n",
        "            train_ids = data_folds[0].get('train', [])\n",
        "            dev_ids = data_folds[0].get('dev', [])\n",
        "            test_ids = data_folds[0].get('test', [])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if train_ids is None or len(train_ids) == 0:\n",
        "        print(\"Splitting data 70/15/15\")\n",
        "        all_ids = list(range(len(labels)))\n",
        "        np.random.shuffle(all_ids)\n",
        "        split1 = int(0.7 * len(all_ids))\n",
        "        split2 = int(0.85 * len(all_ids))\n",
        "        train_ids = all_ids[:split1]\n",
        "        dev_ids = all_ids[split1:split2]\n",
        "        test_ids = all_ids[split2:]\n",
        "\n",
        "    print(f\"Train: {len(train_ids)}, Dev: {len(dev_ids)}, Test: {len(test_ids)}\")\n",
        "\n",
        "    # Fit scalers on training audio/video\n",
        "    train_audio_list = []\n",
        "    for id in train_ids:\n",
        "        try:\n",
        "            audio_data = audio_features[id]\n",
        "            if isinstance(audio_data, dict):\n",
        "                audio_data = audio_data.get('punchline_features',\n",
        "                                           audio_data.get('punchline',\n",
        "                                           list(audio_data.values())[0]))\n",
        "            audio_arr = np.array(audio_data, dtype=np.float32).reshape(-1, 81)\n",
        "            train_audio_list.append(audio_arr)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    train_audio = np.vstack(train_audio_list) if train_audio_list else np.zeros((1, 81))\n",
        "    scaler_audio = StandardScaler().fit(train_audio)\n",
        "\n",
        "    train_video_list = []\n",
        "    for id in train_ids:\n",
        "        try:\n",
        "            video_data = video_features[id]\n",
        "            if isinstance(video_data, dict):\n",
        "                video_data = video_data.get('punchline_features',\n",
        "                                           video_data.get('punchline',\n",
        "                                           list(video_data.values())[0]))\n",
        "            video_arr = np.array(video_data, dtype=np.float32).reshape(-1, 75)\n",
        "            train_video_list.append(video_arr)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    train_video = np.vstack(train_video_list) if train_video_list else np.zeros((1, 75))\n",
        "    scaler_video = StandardScaler().fit(train_video)\n",
        "\n",
        "    # Create datasets\n",
        "    train_set = BertLSTMDataset(train_ids, text_features, audio_features, video_features,\n",
        "                               labels, word_embeddings, scaler_audio, scaler_video, is_train=True)\n",
        "    dev_set = BertLSTMDataset(dev_ids, text_features, audio_features, video_features,\n",
        "                             labels, word_embeddings, scaler_audio, scaler_video, is_train=False)\n",
        "    test_set = BertLSTMDataset(test_ids, text_features, audio_features, video_features,\n",
        "                              labels, word_embeddings, scaler_audio, scaler_video, is_train=False)\n",
        "\n",
        "    return train_set, dev_set, test_set, word_embeddings\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training pipeline with all phases\"\"\"\n",
        "\n",
        "    train_set, dev_set, test_set, word_emb = load_data()\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                             collate_fn=collate_fn_lstm)\n",
        "    dev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, collate_fn=collate_fn_lstm)\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn_lstm)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Phase 1\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PHASE 1: BERT + LSTM AUDIO/VIDEO (Temporal Sequence Modeling)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    model = BertLSTMModel(audio_hidden=64, video_hidden=64, lstm_layers=2).to(DEVICE)\n",
        "\n",
        "    # Layerwise learning rates\n",
        "    param_groups = [\n",
        "        {'params': model.bert_encoder.parameters(), 'lr': 1e-5},\n",
        "        {'params': model.audio_encoder.parameters(), 'lr': 1e-3},\n",
        "        {'params': model.video_encoder.parameters(), 'lr': 1e-3},\n",
        "        {'params': model.fusion.parameters(), 'lr': 1e-3},\n",
        "        {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
        "    ]\n",
        "    opt = optim.AdamW(param_groups, weight_decay=0.01)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
        "\n",
        "    best_f1 = 0\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        loss = train_epoch(model, train_loader, opt, loss_fn, DEVICE)\n",
        "        dev_acc, dev_f1 = evaluate(model, dev_loader, DEVICE)\n",
        "        scheduler.step()\n",
        "\n",
        "        if dev_f1 > best_f1:\n",
        "            best_f1 = dev_f1\n",
        "            torch.save(model.state_dict(), f'{SAVE_FOLDER}/phase1_lstm.pt')\n",
        "\n",
        "        if (epoch + 1) % 3 == 0:\n",
        "            print(f\"Epoch {epoch+1:2d} | Loss: {loss:.4f} | Dev Acc: {dev_acc:.4f} | F1: {dev_f1:.4f}\")\n",
        "\n",
        "    model.load_state_dict(torch.load(f'{SAVE_FOLDER}/phase1_lstm.pt'))\n",
        "    test_acc, test_f1 = evaluate(model, test_loader, DEVICE)\n",
        "    print(f\" Phase 1: Acc={test_acc:.4f}, F1={test_f1:.4f}\\n\")\n",
        "\n",
        "    # Missing Dataset\n",
        "    print(\"=\"*70)\n",
        "    print(\"PHASE 2: ABLATION STUDY (Missing Modalities)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    for missing in [None, 'audio', 'video', 'text']:\n",
        "        acc, f1 = evaluate(model, test_loader, DEVICE, missing)\n",
        "        name = missing if missing else 'All'\n",
        "        print(f\"Missing {name:6s}: Acc={acc:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Knowledge Distillation\n",
        "    print(\"=\"*70)\n",
        "    print(\"PHASE 3: KNOWLEDGE DISTILLATION (Audio Teacher)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Train audio unimodal teacher\n",
        "    print(\"Training audio-only teacher\")\n",
        "    class SimpleAudioTeacher(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.lstm = LSTMAudioEncoder(input_dim=81, hidden_dim=64, num_layers=2)\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(64, 32),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(32, 2)\n",
        "            )\n",
        "\n",
        "        def forward(self, audio, lengths):\n",
        "            audio_feat = self.lstm(audio, lengths)\n",
        "            return self.classifier(audio_feat)\n",
        "\n",
        "    audio_teacher = SimpleAudioTeacher().to(DEVICE)\n",
        "    audio_opt = optim.Adam(audio_teacher.parameters(), lr=1e-3)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        audio_teacher.train()\n",
        "        for batch in train_loader:\n",
        "            audio = batch['audio'].to(DEVICE)\n",
        "            audio_lengths = batch['audio_lengths'].to(DEVICE)\n",
        "            labels = batch['label'].to(DEVICE)\n",
        "\n",
        "            audio_opt.zero_grad()\n",
        "            logits = audio_teacher(audio, audio_lengths)\n",
        "            loss = loss_fn(logits, labels)\n",
        "            loss.backward()\n",
        "            audio_opt.step()\n",
        "\n",
        "    # Train student with KD\n",
        "    print(\"Training student with KD loss\")\n",
        "    model_kd = BertLSTMModel(audio_hidden=64, video_hidden=64, lstm_layers=2).to(DEVICE)\n",
        "\n",
        "    param_groups_kd = [\n",
        "        {'params': model_kd.bert_encoder.parameters(), 'lr': 1e-5},\n",
        "        {'params': model_kd.audio_encoder.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_kd.video_encoder.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_kd.fusion.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_kd.classifier.parameters(), 'lr': 1e-3}\n",
        "    ]\n",
        "    opt_kd = optim.AdamW(param_groups_kd, weight_decay=0.01)\n",
        "    scheduler_kd = optim.lr_scheduler.CosineAnnealingLR(opt_kd, T_max=NUM_EPOCHS)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model_kd.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            word_indices = batch['word_indices'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            audio = batch['audio'].to(DEVICE)\n",
        "            audio_lengths = batch['audio_lengths'].to(DEVICE)\n",
        "            video = batch['video'].to(DEVICE)\n",
        "            video_lengths = batch['video_lengths'].to(DEVICE)\n",
        "            labels = batch['label'].to(DEVICE)\n",
        "\n",
        "            opt_kd.zero_grad()\n",
        "\n",
        "            student_logits = model_kd(word_indices, audio, video,\n",
        "                                      audio_lengths, video_lengths, attention_mask)\n",
        "            ce_loss = loss_fn(student_logits, labels)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = audio_teacher(audio, audio_lengths)\n",
        "\n",
        "            T = 4.0\n",
        "            kd_loss = 0.5 * torch.nn.functional.kl_div(\n",
        "                torch.nn.functional.log_softmax(student_logits/T, dim=1),\n",
        "                torch.softmax(teacher_logits/T, dim=1),\n",
        "                reduction='batchmean'\n",
        "            )\n",
        "\n",
        "            loss = ce_loss + kd_loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model_kd.parameters(), 1.0)\n",
        "            opt_kd.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        scheduler_kd.step()\n",
        "\n",
        "        if (epoch + 1) % 3 == 0:\n",
        "            print(f\"Epoch {epoch+1:2d} | Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    test_acc_kd, test_f1_kd = evaluate(model_kd, test_loader, DEVICE)\n",
        "    print(f\" Phase 3: Acc={test_acc_kd:.4f}, F1={test_f1_kd:.4f}\\n\")\n",
        "\n",
        "    # Results\n",
        "    print(f\"Phase 1 (LSTM Baseline): Acc={test_acc:.4f}, F1={test_f1:.4f}\")\n",
        "    print(f\"Phase 3 (KD): Acc={test_acc_kd:.4f}, F1={test_f1_kd:.4f}\")\n",
        "\n",
        "    print(f\"\\nModels saved to: {SAVE_FOLDER}/\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDm0UMX50hQa",
        "outputId": "88b8e276-c682-41ca-cc80-88aaff2dd12e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-12-29 08:32:41--  https://www.dropbox.com/s/izk6khkrdwcncia/ted_humor_sdk_v1.zip?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/002rz175n5ferwyvif2hv/ted_humor_sdk_v1.zip?rlkey=r8bszra1ez6zedylbx1d4wm99&dl=1 [following]\n",
            "--2025-12-29 08:32:42--  https://www.dropbox.com/scl/fi/002rz175n5ferwyvif2hv/ted_humor_sdk_v1.zip?rlkey=r8bszra1ez6zedylbx1d4wm99&dl=1\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc5d6abf8e1da565817b8b7131c6.dl.dropboxusercontent.com/cd/0/inline/C3-68fuzf7TwdRf1gofoJuEiQxA3g2WxCPKMvA2JweobVIWyI1sDyOf6gbt2vgOlXxn_Zem9PHQBioxDLeteO_8XqBYjKJCS1RYwXJwmMYLhjW4Ee4FhlvEbefIRVpMRfJ3faP7LKCQAzs4Cu_iZlVyM/file?dl=1# [following]\n",
            "--2025-12-29 08:32:42--  https://uc5d6abf8e1da565817b8b7131c6.dl.dropboxusercontent.com/cd/0/inline/C3-68fuzf7TwdRf1gofoJuEiQxA3g2WxCPKMvA2JweobVIWyI1sDyOf6gbt2vgOlXxn_Zem9PHQBioxDLeteO_8XqBYjKJCS1RYwXJwmMYLhjW4Ee4FhlvEbefIRVpMRfJ3faP7LKCQAzs4Cu_iZlVyM/file?dl=1\n",
            "Resolving uc5d6abf8e1da565817b8b7131c6.dl.dropboxusercontent.com (uc5d6abf8e1da565817b8b7131c6.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uc5d6abf8e1da565817b8b7131c6.dl.dropboxusercontent.com (uc5d6abf8e1da565817b8b7131c6.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/C39xVm8Uid2kxsE6d8IP4MXy6qPOcL230kvuaHodjrLrOCFRhHqm0nVVnswT4mQZosDDTSWYrN1gkzzK0XBiiJlTXCZw3hw8BIG8SQfBHMblmDKpQfSLbKRKMUVFMSHj0aRcp1RuHJ10Q-w1GT3bcyMTsqJapr3hTF8C8f5aBEiR31T6Z7UPLj7Y_fZeu4Wyifyz8PEheGoN95sOHwOGLmwj6ar8IpL-VOYfkKCLXqZ2SfYyDFmgcgPMxViKZOgHoE7k_tMgbXcanjLhUqzs0I-28FI_YDh-NxNzx8WgZ1Ka4CYwv7OdJdh5QjKwxoZnNVYgCUid2wP_9qKBSlkZMVKiw93JOF1ExqRoBw5uzwRvb9EZXUQ6eUFLsKZAzoLtWm4/file?dl=1 [following]\n",
            "--2025-12-29 08:32:43--  https://uc5d6abf8e1da565817b8b7131c6.dl.dropboxusercontent.com/cd/0/inline2/C39xVm8Uid2kxsE6d8IP4MXy6qPOcL230kvuaHodjrLrOCFRhHqm0nVVnswT4mQZosDDTSWYrN1gkzzK0XBiiJlTXCZw3hw8BIG8SQfBHMblmDKpQfSLbKRKMUVFMSHj0aRcp1RuHJ10Q-w1GT3bcyMTsqJapr3hTF8C8f5aBEiR31T6Z7UPLj7Y_fZeu4Wyifyz8PEheGoN95sOHwOGLmwj6ar8IpL-VOYfkKCLXqZ2SfYyDFmgcgPMxViKZOgHoE7k_tMgbXcanjLhUqzs0I-28FI_YDh-NxNzx8WgZ1Ka4CYwv7OdJdh5QjKwxoZnNVYgCUid2wP_9qKBSlkZMVKiw93JOF1ExqRoBw5uzwRvb9EZXUQ6eUFLsKZAzoLtWm4/file?dl=1\n",
            "Reusing existing connection to uc5d6abf8e1da565817b8b7131c6.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 801445658 (764M) [application/binary]\n",
            "Saving to: ‘ted_humor_sdk_v1.zip?dl=1’\n",
            "\n",
            "ted_humor_sdk_v1.zi 100%[===================>] 764.32M   111MB/s    in 7.1s    \n",
            "\n",
            "2025-12-29 08:32:50 (107 MB/s) - ‘ted_humor_sdk_v1.zip?dl=1’ saved [801445658/801445658]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.dropbox.com/s/izk6khkrdwcncia/ted_humor_sdk_v1.zip?dl=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBlFsyVd0y9y",
        "outputId": "2e3e76cc-a092-4d34-aef0-4bcdbc70cc7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  ted_humor_sdk_v1.zip?dl=1\n",
            "   creating: final_humor_sdk/\n",
            "  inflating: final_humor_sdk/word_embedding_list.pkl  \n",
            "  inflating: final_humor_sdk/data_folds.pkl  \n",
            "  inflating: final_humor_sdk/humor_label_sdk.pkl  \n",
            "  inflating: final_humor_sdk/covarep_features_sdk.pkl  \n",
            "  inflating: final_humor_sdk/openface_features_sdk.pkl  \n",
            "  inflating: final_humor_sdk/word_embedding_indexes_sdk.pkl  \n"
          ]
        }
      ],
      "source": [
        "!unzip ted_humor_sdk_v1.zip?dl=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlgSWAELgpwI",
        "outputId": "5cef420c-bb61-4942-8cf3-5188b7eb624e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Train: 10598, Dev: 2626, Test: 3290\n",
            "Epoch  1/20 | Loss: 0.7227 | Dev Acc: 0.5141 | F1: 0.4944 |  Saved\n",
            "Epoch  3/20 | Loss: 0.7083 | Dev Acc: 0.5175 | F1: 0.4963 |  Saved\n",
            "Epoch  6/20 | Loss: 0.6955 | Dev Acc: 0.5560 | F1: 0.5496 |  Saved\n",
            "Epoch  9/20 | Loss: 0.6719 | Dev Acc: 0.5819 | F1: 0.5815 |  Saved\n",
            "Epoch 12/20 | Loss: 0.6452 | Dev Acc: 0.5952 | F1: 0.5950 |  Saved\n",
            "Epoch 15/20 | Loss: 0.6046 | Dev Acc: 0.5971 | F1: 0.5945 | \n",
            "Epoch 18/20 | Loss: 0.5694 | Dev Acc: 0.5929 | F1: 0.5876 | \n",
            "\n",
            " Phase 1 - All Modalities: Acc=0.6067, F1=0.6064\n",
            "\n",
            "Missing ALL   : Accuracy=0.6067, F1=0.6064\n",
            "Missing AUDIO : Accuracy=0.5492, F1=0.5193\n",
            "Missing VIDEO : Accuracy=0.6027, F1=0.6024\n",
            "Missing TEXT  : Accuracy=0.5802, F1=0.5796\n",
            "\n",
            "Training audio teacher\n",
            "Training student with KD\n",
            "Epoch  1/20 | Loss: 0.7272 | Dev Acc: 0.4966 | F1: 0.3485 |  Saved\n",
            "Epoch  3/20 | Loss: 0.7128 | Dev Acc: 0.5061 | F1: 0.4532 | \n",
            "Epoch  6/20 | Loss: 0.7053 | Dev Acc: 0.4992 | F1: 0.4716 | \n",
            "Epoch  9/20 | Loss: 0.6945 | Dev Acc: 0.5248 | F1: 0.4482 | \n",
            "Epoch 12/20 | Loss: 0.6841 | Dev Acc: 0.5484 | F1: 0.5415 | \n",
            "Epoch 15/20 | Loss: 0.6687 | Dev Acc: 0.5918 | F1: 0.5705 | \n",
            "Epoch 18/20 | Loss: 0.6572 | Dev Acc: 0.5788 | F1: 0.5345 | \n",
            "\n",
            " Phase 3 - KD: Acc=0.5799, F1=0.5767\n",
            "\n",
            "Epoch  1/20 | Loss: 0.7210 | Dev Acc: 0.4947 | F1: 0.4925 |  Saved\n",
            "Epoch  3/20 | Loss: 0.7092 | Dev Acc: 0.5122 | F1: 0.4360 | \n",
            "Epoch  6/20 | Loss: 0.6995 | Dev Acc: 0.5331 | F1: 0.5331 |  Saved\n",
            "Epoch  9/20 | Loss: 0.6893 | Dev Acc: 0.5590 | F1: 0.5516 | \n",
            "Epoch 12/20 | Loss: 0.6782 | Dev Acc: 0.5720 | F1: 0.5422 | \n",
            "Epoch 15/20 | Loss: 0.6692 | Dev Acc: 0.5823 | F1: 0.5817 | \n",
            "Epoch 18/20 | Loss: 0.6556 | Dev Acc: 0.5967 | F1: 0.5810 | \n",
            "\n",
            " Phase 4 - Calibration: Acc=0.5979, F1=0.5970\n",
            "\n",
            "Phase 4 Missing Data:\n",
            "  Missing ALL   : Accuracy=0.5979, F1=0.5970\n",
            "  Missing AUDIO : Accuracy=0.5498, F1=0.5277\n",
            "  Missing VIDEO : Accuracy=0.6036, F1=0.6029\n",
            "  Missing TEXT  : Accuracy=0.5769, F1=0.5635\n",
            "\n",
            "\n",
            " Result:\n",
            "Phase 1 (Baseline):      Acc=0.6067, F1=0.6064\n",
            "Phase 3 (KD):            Acc=0.5799, F1=0.5767\n",
            "Phase 4 (Calibration):   Acc=0.5979, F1=0.5970\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Key Settings\n",
        "BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 20\n",
        "LEARNING_RATE = 2e-5\n",
        "TEXT_DROPOUT = 0.8\n",
        "DATA_FOLDER = './ted_humor_data'\n",
        "SAVE_FOLDER = './saved_models'\n",
        "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
        "\n",
        "# 1. Dataset with raw sequences\n",
        "\n",
        "class BertLSTMDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ids, text_data, audio_data, video_data, labels,\n",
        "                 word_embeddings, audio_scaler, video_scaler,\n",
        "                 is_train=False, text_dropout=0.0):\n",
        "        self.ids = ids\n",
        "        self.text_data = text_data\n",
        "        self.audio_data = audio_data\n",
        "        self.video_data = video_data\n",
        "        self.labels = labels\n",
        "        self.word_embeddings = word_embeddings\n",
        "        self.audio_scaler = audio_scaler\n",
        "        self.video_scaler = video_scaler\n",
        "        self.is_train = is_train\n",
        "        self.text_dropout = text_dropout\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_id = self.ids[idx]\n",
        "\n",
        "        # Get Text\n",
        "        text_data = self.text_data[sample_id]\n",
        "        if isinstance(text_data, dict):\n",
        "            if 'punchline_features' in text_data:\n",
        "                text_indices = text_data['punchline_features']\n",
        "            elif 'punchline' in text_data:\n",
        "                text_indices = text_data['punchline']\n",
        "            else:\n",
        "                text_indices = list(text_data.values())[0]\n",
        "        else:\n",
        "            text_indices = text_data\n",
        "\n",
        "        word_indices = []\n",
        "        if isinstance(text_indices, (list, np.ndarray)):\n",
        "            for idx_list in text_indices:\n",
        "                try:\n",
        "                    if isinstance(idx_list, (list, np.ndarray)) and len(idx_list) > 0:\n",
        "                        idx_val = int(idx_list[0]) if hasattr(idx_list[0], '__int__') else 0\n",
        "                    else:\n",
        "                        idx_val = int(idx_list) if hasattr(idx_list, '__int__') else 0\n",
        "\n",
        "                    if 0 <= idx_val <= 30522:\n",
        "                        word_indices.append(idx_val)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        if len(word_indices) == 0:\n",
        "            word_indices = [101]\n",
        "\n",
        "        word_indices = word_indices[:512]\n",
        "\n",
        "        # Get Audio - Raw Sequence\n",
        "        audio_data = self.audio_data[sample_id]\n",
        "        if isinstance(audio_data, dict):\n",
        "            if 'punchline_features' in audio_data:\n",
        "                audio_features = audio_data['punchline_features']\n",
        "            elif 'punchline' in audio_data:\n",
        "                audio_features = audio_data['punchline']\n",
        "            else:\n",
        "                audio_features = list(audio_data.values())[0]\n",
        "        else:\n",
        "            audio_features = audio_data\n",
        "\n",
        "        try:\n",
        "            audio_raw = np.array(audio_features, dtype=np.float32).reshape(-1, 81)\n",
        "            if self.audio_scaler:\n",
        "                audio_raw = self.audio_scaler.transform(audio_raw)\n",
        "            audio_seq = audio_raw[:50]  # Cap at 50 timesteps\n",
        "        except:\n",
        "            audio_seq = np.zeros((1, 81), dtype=np.float32)\n",
        "\n",
        "        # Get Video - Raw Sequence\n",
        "\n",
        "        video_data = self.video_data[sample_id]\n",
        "        if isinstance(video_data, dict):\n",
        "            if 'punchline_features' in video_data:\n",
        "                video_features = video_data['punchline_features']\n",
        "            elif 'punchline' in video_data:\n",
        "                video_features = video_data['punchline']\n",
        "            else:\n",
        "                video_features = list(video_data.values())[0]\n",
        "        else:\n",
        "            video_features = video_data\n",
        "\n",
        "        try:\n",
        "            video_raw = np.array(video_features, dtype=np.float32).reshape(-1, 75)\n",
        "            if self.video_scaler:\n",
        "                video_raw = self.video_scaler.transform(video_raw)\n",
        "            video_seq = video_raw[:50]\n",
        "        except:\n",
        "            video_seq = np.zeros((1, 75), dtype=np.float32)\n",
        "\n",
        "        if self.is_train and np.random.rand() < self.text_dropout:\n",
        "            word_indices = [101]\n",
        "\n",
        "        label = self.labels[sample_id]\n",
        "\n",
        "        return {\n",
        "            'word_indices': torch.LongTensor(word_indices),\n",
        "            'audio': torch.FloatTensor(audio_seq),  # (seq_len, 81)\n",
        "            'video': torch.FloatTensor(video_seq),  # (seq_len, 75)\n",
        "            'audio_len': len(audio_seq),\n",
        "            'video_len': len(video_seq),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn_lstm(batch):\n",
        "    \"\"\"Collate function for variable-length sequences\"\"\"\n",
        "    # Pad text\n",
        "    max_text_len = max(len(item['word_indices']) for item in batch)\n",
        "    word_indices_padded = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for item in batch:\n",
        "        indices = item['word_indices'].numpy()\n",
        "        padded = np.pad(indices, (0, max_text_len - len(indices)), constant_values=0)\n",
        "        word_indices_padded.append(torch.LongTensor(padded))\n",
        "\n",
        "        mask = np.ones(len(indices))\n",
        "        mask_padded = np.pad(mask, (0, max_text_len - len(indices)), constant_values=0)\n",
        "        attention_masks.append(torch.LongTensor(mask_padded))\n",
        "\n",
        "    # Pad audio\n",
        "    max_audio_len = max(len(item['audio']) for item in batch)\n",
        "    audio_padded = []\n",
        "    audio_lengths = []\n",
        "\n",
        "    for item in batch:\n",
        "        audio = item['audio'].numpy()\n",
        "        if len(audio) < max_audio_len:\n",
        "            audio = np.pad(audio, ((0, max_audio_len - len(audio)), (0, 0)), constant_values=0)\n",
        "        audio_padded.append(torch.FloatTensor(audio))\n",
        "        audio_lengths.append(item['audio_len'])\n",
        "\n",
        "    # Pad video\n",
        "    max_video_len = max(len(item['video']) for item in batch)\n",
        "    video_padded = []\n",
        "    video_lengths = []\n",
        "\n",
        "    for item in batch:\n",
        "        video = item['video'].numpy()\n",
        "        if len(video) < max_video_len:\n",
        "            video = np.pad(video, ((0, max_video_len - len(video)), (0, 0)), constant_values=0)\n",
        "        video_padded.append(torch.FloatTensor(video))\n",
        "        video_lengths.append(item['video_len'])\n",
        "\n",
        "    return {\n",
        "        'word_indices': torch.stack(word_indices_padded),\n",
        "        'attention_mask': torch.stack(attention_masks),\n",
        "        'audio': torch.stack(audio_padded),  # (batch, max_len, 81)\n",
        "        'video': torch.stack(video_padded),  # (batch, max_len, 75)\n",
        "        'audio_lengths': torch.LongTensor(audio_lengths),\n",
        "        'video_lengths': torch.LongTensor(video_lengths),\n",
        "        'label': torch.stack([item['label'] for item in batch])\n",
        "    }\n",
        "\n",
        "\n",
        "# 2. Encoder\n",
        "\n",
        "class LSTMTextEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size=30522, embedding_dim=300, hidden_dim=384, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim,\n",
        "                           num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "        self.text_dim = hidden_dim * 2\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids: (batch, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            text_out: (batch, text_dim) - final hidden state\n",
        "        \"\"\"\n",
        "        if input_ids.shape[0] == 0 or input_ids.shape[1] == 0:\n",
        "            return torch.zeros(max(input_ids.shape[0], 1), self.text_dim).to(input_ids.device)\n",
        "\n",
        "        embedded = self.embedding(input_ids)\n",
        "\n",
        "        _, (hidden, cell) = self.lstm(embedded)\n",
        "\n",
        "        text_out = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
        "\n",
        "        return text_out\n",
        "\n",
        "\n",
        "class LSTMAudioEncoder(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, input_dim=81, hidden_dim=64, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim,\n",
        "                           num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, audio_seq, audio_lengths):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            audio_seq: (batch, max_seq_len, 81)\n",
        "            audio_lengths: (batch,)\n",
        "\n",
        "        Returns:\n",
        "            audio_out: (batch, hidden_dim) - final hidden state\n",
        "        \"\"\"\n",
        "        if audio_seq.shape[0] == 0 or audio_seq.shape[1] == 0:\n",
        "            return torch.zeros(audio_seq.shape[0], self.lstm.hidden_size).to(audio_seq.device)\n",
        "\n",
        "        _, (hidden, cell) = self.lstm(audio_seq)\n",
        "\n",
        "        audio_out = hidden[-1]\n",
        "\n",
        "        return audio_out\n",
        "\n",
        "\n",
        "class LSTMVideoEncoder(nn.Module):\n",
        "    \"\"\"LSTM encoder for video - raw sequence → final hidden state\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=75, hidden_dim=64, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim,\n",
        "                           num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, video_seq, video_lengths):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            video_seq: (batch, max_seq_len, 75)\n",
        "            video_lengths: (batch,)\n",
        "\n",
        "        Returns:\n",
        "            video_out: (batch, hidden_dim) - final hidden state\n",
        "        \"\"\"\n",
        "        if video_seq.shape[0] == 0 or video_seq.shape[1] == 0:\n",
        "            return torch.zeros(video_seq.shape[0], self.lstm.hidden_size).to(video_seq.device)\n",
        "\n",
        "        _, (hidden, cell) = self.lstm(video_seq)\n",
        "\n",
        "        video_out = hidden[-1]\n",
        "\n",
        "        return video_out\n",
        "\n",
        "\n",
        "# Gated Fusion\n",
        "\n",
        "class GatedFusion(nn.Module):\n",
        "\n",
        "    def __init__(self, text_dim=768, audio_dim=64, video_dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        self.audio_gate = nn.Sequential(\n",
        "            nn.Linear(audio_dim + text_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, audio_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.video_gate = nn.Sequential(\n",
        "            nn.Linear(video_dim + text_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, video_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.text_proj = nn.Linear(text_dim, 256)\n",
        "        self.audio_proj = nn.Linear(audio_dim, 256)\n",
        "        self.video_proj = nn.Linear(video_dim, 256)\n",
        "\n",
        "    def forward(self, text_feat, audio_feat, video_feat):\n",
        "        # Compute gates\n",
        "        audio_input = torch.cat([audio_feat, text_feat], dim=1)\n",
        "        audio_gate = self.audio_gate(audio_input)\n",
        "        audio_gated = audio_feat * audio_gate\n",
        "\n",
        "        video_input = torch.cat([video_feat, text_feat], dim=1)\n",
        "        video_gate = self.video_gate(video_input)\n",
        "        video_gated = video_feat * video_gate\n",
        "\n",
        "        # Project and fuse\n",
        "        text_proj = self.text_proj(text_feat)\n",
        "        audio_proj = self.audio_proj(audio_gated)\n",
        "        video_proj = self.video_proj(video_gated)\n",
        "\n",
        "        fused = text_proj + audio_proj + video_proj\n",
        "\n",
        "        return fused\n",
        "\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    \"\"\"Align audio/video to text\"\"\"\n",
        "\n",
        "    def __init__(self, dim=64):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(dim, dim)\n",
        "        self.key = nn.Linear(dim, dim)\n",
        "        self.value = nn.Linear(dim, dim)\n",
        "        self.out_proj = nn.Linear(dim, dim)\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, text_feat_small, modality_feat):\n",
        "        q = self.query(text_feat_small)\n",
        "        k = self.key(modality_feat)\n",
        "        v = self.value(modality_feat)\n",
        "\n",
        "        scores = torch.matmul(q.unsqueeze(1), k.unsqueeze(2)) / (self.dim ** 0.5)\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        output = torch.matmul(weights, v.unsqueeze(1)).squeeze(1)\n",
        "        output = self.out_proj(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# Main Model\n",
        "\n",
        "class ImprovedBertLSTMModel(nn.Module):\n",
        "    \"\"\"Complete multimodal model with raw sequences\"\"\"\n",
        "\n",
        "    def __init__(self, audio_hidden=64, video_hidden=64, lstm_layers=2, text_hidden=384):\n",
        "        super().__init__()\n",
        "\n",
        "        # LSTM Text encoder\n",
        "        self.text_encoder = LSTMTextEncoder(embedding_dim=300, hidden_dim=text_hidden,\n",
        "                                           num_layers=lstm_layers)\n",
        "        text_dim = self.text_encoder.text_dim\n",
        "\n",
        "        # LSTM Audio encoder\n",
        "        self.audio_encoder = LSTMAudioEncoder(input_dim=81,\n",
        "                                             hidden_dim=audio_hidden,\n",
        "                                             num_layers=lstm_layers)\n",
        "        audio_dim = audio_hidden\n",
        "\n",
        "        # LSTM Video encoder\n",
        "        self.video_encoder = LSTMVideoEncoder(input_dim=75,\n",
        "                                             hidden_dim=video_hidden,\n",
        "                                             num_layers=lstm_layers)\n",
        "        video_dim = video_hidden\n",
        "\n",
        "        # Cross-modal attention\n",
        "        self.audio_attention = CrossModalAttention(dim=audio_hidden)\n",
        "        self.video_attention = CrossModalAttention(dim=video_hidden)\n",
        "        self.text_small_proj = nn.Linear(text_dim, audio_hidden)\n",
        "\n",
        "        # Gated Fusion\n",
        "        self.gated_fusion = GatedFusion(text_dim=text_dim,\n",
        "                                       audio_dim=audio_dim,\n",
        "                                       video_dim=video_dim)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, word_indices, audio, video,\n",
        "               audio_lengths, video_lengths, attention_mask=None):\n",
        "\n",
        "        # Encode: raw sequences → final hidden states\n",
        "        text_out = self.text_encoder(word_indices, attention_mask)\n",
        "        audio_out = self.audio_encoder(audio, audio_lengths)\n",
        "        video_out = self.video_encoder(video, video_lengths)\n",
        "\n",
        "        # Cross-modal attentio\n",
        "        text_small = self.text_small_proj(text_out)\n",
        "        audio_aligned = self.audio_attention(text_small, audio_out)\n",
        "        video_aligned = self.video_attention(text_small, video_out)\n",
        "\n",
        "        # Gated fusion: final hidden states → fused representation\n",
        "        fused = self.gated_fusion(text_out, audio_aligned, video_aligned)  # (batch, 256)\n",
        "\n",
        "        # Classify\n",
        "        logits = self.classifier(fused)  # (batch, 2)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "class SimpleTeacher(nn.Module):\n",
        "    \"\"\"Simple teacher for knowledge distillation\"\"\"\n",
        "\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.out = nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.enc(x)\n",
        "        return self.out(x)\n",
        "\n",
        "\n",
        "# Training and Evaluation\n",
        "\n",
        "def train_epoch(model, loader, optimizer, loss_fn, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        word_indices = batch['word_indices'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        audio = batch['audio'].to(device)\n",
        "        video = batch['video'].to(device)\n",
        "        audio_lengths = batch['audio_lengths'].to(device)\n",
        "        video_lengths = batch['video_lengths'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(word_indices, audio, video,\n",
        "                      audio_lengths, video_lengths,\n",
        "                      attention_mask=attention_mask)\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "def evaluate(model, loader, device, missing_mod=None):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            word_indices = batch['word_indices'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            audio = batch['audio'].to(device)\n",
        "            video = batch['video'].to(device)\n",
        "            audio_lengths = batch['audio_lengths'].to(device)\n",
        "            video_lengths = batch['video_lengths'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            if missing_mod == 'audio':\n",
        "                audio = torch.zeros_like(audio)\n",
        "                audio_lengths = torch.ones_like(audio_lengths)\n",
        "            elif missing_mod == 'video':\n",
        "                video = torch.zeros_like(video)\n",
        "                video_lengths = torch.ones_like(video_lengths)\n",
        "            elif missing_mod == 'text':\n",
        "                word_indices = torch.zeros_like(word_indices)\n",
        "                attention_mask = torch.zeros_like(attention_mask)\n",
        "\n",
        "            logits = model(word_indices, audio, video,\n",
        "                          audio_lengths, video_lengths,\n",
        "                          attention_mask=attention_mask)\n",
        "            pred = logits.argmax(dim=1)\n",
        "\n",
        "            preds.extend(pred.cpu().numpy())\n",
        "            targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(targets, preds)\n",
        "    f1 = f1_score(targets, preds, average='macro', zero_division=0)\n",
        "    return acc, f1\n",
        "\n",
        "\n",
        "# Data Loading\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Load TED-Humor dataset\"\"\"\n",
        "\n",
        "\n",
        "    with open(f'{DATA_FOLDER}/data_folds.pkl', 'rb') as f:\n",
        "        data_folds = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    with open(f'{DATA_FOLDER}/word_embeddings_list.pkl', 'rb') as f:\n",
        "        word_embeddings = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    with open(f'{DATA_FOLDER}/word_embedding_indexes_sdk.pkl', 'rb') as f:\n",
        "        text_features = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    with open(f'{DATA_FOLDER}/covarep_features_sdk.pkl', 'rb') as f:\n",
        "        audio_features = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    with open(f'{DATA_FOLDER}/openface_features_sdk.pkl', 'rb') as f:\n",
        "        video_features = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    with open(f'{DATA_FOLDER}/humor_labels_sdk.pkl', 'rb') as f:\n",
        "        labels = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    # Get splits\n",
        "    train_ids = None\n",
        "    try:\n",
        "        if isinstance(data_folds, dict) and 'train' in data_folds:\n",
        "            train_ids = data_folds['train']\n",
        "            dev_ids = data_folds['dev']\n",
        "            test_ids = data_folds['test']\n",
        "        elif isinstance(data_folds, list):\n",
        "            train_ids = data_folds[0].get('train')\n",
        "            dev_ids = data_folds[0].get('dev')\n",
        "            test_ids = data_folds[0].get('test')\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if train_ids is None or len(train_ids) == 0:\n",
        "\n",
        "        all_ids = list(range(len(labels)))\n",
        "        np.random.shuffle(all_ids)\n",
        "        split1 = int(0.7 * len(all_ids))\n",
        "        split2 = int(0.85 * len(all_ids))\n",
        "        train_ids = all_ids[:split1]\n",
        "        dev_ids = all_ids[split1:split2]\n",
        "        test_ids = all_ids[split2:]\n",
        "\n",
        "    print(f\"Train: {len(train_ids)}, Dev: {len(dev_ids)}, Test: {len(test_ids)}\")\n",
        "\n",
        "    # Scale audio/video\n",
        "    train_audio_list = []\n",
        "    for id in train_ids:\n",
        "        try:\n",
        "            audio_data = audio_features[id]\n",
        "            if isinstance(audio_data, dict):\n",
        "                audio_data = audio_data.get('punchline_features',\n",
        "                                           audio_data.get('punchline', list(audio_data.values())[0]))\n",
        "            audio_arr = np.array(audio_data, dtype=np.float32).reshape(-1, 81)\n",
        "            train_audio_list.append(audio_arr)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    train_audio = np.vstack(train_audio_list) if train_audio_list else np.zeros((1, 81))\n",
        "    scaler_audio = StandardScaler().fit(train_audio)\n",
        "\n",
        "    train_video_list = []\n",
        "    for id in train_ids:\n",
        "        try:\n",
        "            video_data = video_features[id]\n",
        "            if isinstance(video_data, dict):\n",
        "                video_data = video_data.get('punchline_features',\n",
        "                                           video_data.get('punchline', list(video_data.values())[0]))\n",
        "            video_arr = np.array(video_data, dtype=np.float32).reshape(-1, 75)\n",
        "            train_video_list.append(video_arr)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    train_video = np.vstack(train_video_list) if train_video_list else np.zeros((1, 75))\n",
        "    scaler_video = StandardScaler().fit(train_video)\n",
        "\n",
        "    # Create datasets\n",
        "    train_set = BertLSTMDataset(train_ids, text_features, audio_features,\n",
        "                               video_features, labels, word_embeddings,\n",
        "                               scaler_audio, scaler_video, is_train=True,\n",
        "                               text_dropout=TEXT_DROPOUT)\n",
        "\n",
        "    dev_set = BertLSTMDataset(dev_ids, text_features, audio_features,\n",
        "                             video_features, labels, word_embeddings,\n",
        "                             scaler_audio, scaler_video, is_train=False,\n",
        "                             text_dropout=0.0)\n",
        "\n",
        "    test_set = BertLSTMDataset(test_ids, text_features, audio_features,\n",
        "                              video_features, labels, word_embeddings,\n",
        "                              scaler_audio, scaler_video, is_train=False,\n",
        "                              text_dropout=0.0)\n",
        "\n",
        "    return train_set, dev_set, test_set, word_embeddings\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Load data\n",
        "    train_set, dev_set, test_set, _ = load_data()\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_fn_lstm)\n",
        "    dev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE,\n",
        "                           collate_fn=collate_fn_lstm)\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE,\n",
        "                            collate_fn=collate_fn_lstm)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Phase 1: Baseline\n",
        "\n",
        "\n",
        "    model_phase1 = ImprovedBertLSTMModel(audio_hidden=64, video_hidden=64,\n",
        "                                         lstm_layers=2, text_hidden=384).to(DEVICE)\n",
        "\n",
        "    param_groups = [\n",
        "        {'params': model_phase1.text_encoder.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase1.audio_encoder.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase1.video_encoder.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase1.audio_attention.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase1.video_attention.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase1.text_small_proj.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase1.gated_fusion.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase1.classifier.parameters(), 'lr': 1e-3}\n",
        "    ]\n",
        "\n",
        "    optimizer_p1 = optim.AdamW(param_groups, weight_decay=0.01)\n",
        "    scheduler_p1 = optim.lr_scheduler.CosineAnnealingLR(optimizer_p1,\n",
        "                                                        T_max=NUM_EPOCHS,\n",
        "                                                        eta_min=1e-6)\n",
        "\n",
        "    best_f1 = 0\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        loss = train_epoch(model_phase1, train_loader, optimizer_p1, loss_fn, DEVICE)\n",
        "        dev_acc, dev_f1 = evaluate(model_phase1, dev_loader, DEVICE)\n",
        "        scheduler_p1.step()\n",
        "\n",
        "        if dev_f1 > best_f1:\n",
        "            best_f1 = dev_f1\n",
        "            torch.save(model_phase1.state_dict(),\n",
        "                      f'{SAVE_FOLDER}/phase1_baseline.pt')\n",
        "            status = \" Saved\"\n",
        "        else:\n",
        "            status = \"\"\n",
        "\n",
        "        if (epoch + 1) % 3 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1:2d}/{NUM_EPOCHS} | Loss: {loss:.4f} | \"\n",
        "                  f\"Dev Acc: {dev_acc:.4f} | F1: {dev_f1:.4f} | {status}\")\n",
        "\n",
        "    model_phase1.load_state_dict(torch.load(f'{SAVE_FOLDER}/phase1_baseline.pt'))\n",
        "    test_acc_p1, test_f1_p1 = evaluate(model_phase1, test_loader, DEVICE)\n",
        "    print(f\"\\n Phase 1 - All Modalities: Acc={test_acc_p1:.4f}, F1={test_f1_p1:.4f}\\n\")\n",
        "\n",
        "    # Phase 2: Missing Dataset\n",
        "\n",
        "\n",
        "\n",
        "    phase2_results = {}\n",
        "    for missing in [None, 'audio', 'video', 'text']:\n",
        "        acc, f1 = evaluate(model_phase1, test_loader, DEVICE, missing)\n",
        "        name = missing.upper() if missing else 'ALL'\n",
        "        phase2_results[name] = (acc, f1)\n",
        "        print(f\"Missing {name:6s}: Accuracy={acc:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Phase 3: Knowledge Distillation\n",
        "\n",
        "\n",
        "    print(\"Training audio teacher\")\n",
        "    audio_teacher = SimpleTeacher(input_size=81).to(DEVICE)\n",
        "    audio_opt = optim.Adam(audio_teacher.parameters(), lr=1e-3)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        audio_teacher.train()\n",
        "        for batch in train_loader:\n",
        "            audio = batch['audio'].to(DEVICE)\n",
        "            # Mean pool audio for teacher\n",
        "            audio_mean = audio.mean(dim=1)  # (batch, 81)\n",
        "            labels = batch['label'].to(DEVICE)\n",
        "            audio_opt.zero_grad()\n",
        "            logits = audio_teacher(audio_mean)\n",
        "            loss = loss_fn(logits, labels)\n",
        "            loss.backward()\n",
        "            audio_opt.step()\n",
        "\n",
        "    print(\"Training student with KD\")\n",
        "    model_phase3 = ImprovedBertLSTMModel(audio_hidden=64, video_hidden=64,\n",
        "                                         lstm_layers=2, text_hidden=384).to(DEVICE)\n",
        "\n",
        "    param_groups_p3 = [\n",
        "        {'params': model_phase3.text_encoder.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase3.audio_encoder.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase3.video_encoder.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase3.audio_attention.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase3.video_attention.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase3.text_small_proj.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase3.gated_fusion.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase3.classifier.parameters(), 'lr': 1e-3}\n",
        "    ]\n",
        "\n",
        "    optimizer_p3 = optim.AdamW(param_groups_p3, weight_decay=0.01)\n",
        "    scheduler_p3 = optim.lr_scheduler.CosineAnnealingLR(optimizer_p3,\n",
        "                                                        T_max=NUM_EPOCHS,\n",
        "                                                        eta_min=1e-6)\n",
        "\n",
        "    best_f1_p3 = 0\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model_phase3.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            word_indices = batch['word_indices'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            audio = batch['audio'].to(DEVICE)\n",
        "            video = batch['video'].to(DEVICE)\n",
        "            audio_lengths = batch['audio_lengths'].to(DEVICE)\n",
        "            video_lengths = batch['video_lengths'].to(DEVICE)\n",
        "            labels = batch['label'].to(DEVICE)\n",
        "\n",
        "            optimizer_p3.zero_grad()\n",
        "\n",
        "            # Student prediction\n",
        "            student_logits = model_phase3(word_indices, audio, video,\n",
        "                                         audio_lengths, video_lengths,\n",
        "                                         attention_mask=attention_mask)\n",
        "            ce_loss = loss_fn(student_logits, labels)\n",
        "\n",
        "            # Teacher prediction (mean pool audio)\n",
        "            audio_mean = audio.mean(dim=1)\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = audio_teacher(audio_mean)\n",
        "\n",
        "            # KD loss\n",
        "            T = 4.0\n",
        "            kd_loss = 0.5 * F.kl_div(\n",
        "                F.log_softmax(student_logits / T, dim=1),\n",
        "                F.softmax(teacher_logits / T, dim=1),\n",
        "                reduction='batchmean'\n",
        "            )\n",
        "\n",
        "            loss = ce_loss + kd_loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model_phase3.parameters(), 1.0)\n",
        "            optimizer_p3.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        dev_acc, dev_f1 = evaluate(model_phase3, dev_loader, DEVICE)\n",
        "        scheduler_p3.step()\n",
        "\n",
        "        if dev_f1 > best_f1_p3:\n",
        "            best_f1_p3 = dev_f1\n",
        "            torch.save(model_phase3.state_dict(),\n",
        "                      f'{SAVE_FOLDER}/phase3_kd.pt')\n",
        "            status = \" Saved\"\n",
        "        else:\n",
        "            status = \"\"\n",
        "\n",
        "        if (epoch + 1) % 3 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1:2d}/{NUM_EPOCHS} | Loss: {total_loss/len(train_loader):.4f} | \"\n",
        "                  f\"Dev Acc: {dev_acc:.4f} | F1: {dev_f1:.4f} | {status}\")\n",
        "\n",
        "    model_phase3.load_state_dict(torch.load(f'{SAVE_FOLDER}/phase3_kd.pt'))\n",
        "    test_acc_p3, test_f1_p3 = evaluate(model_phase3, test_loader, DEVICE)\n",
        "    print(f\"\\n Phase 3 - KD: Acc={test_acc_p3:.4f}, F1={test_f1_p3:.4f}\\n\")\n",
        "\n",
        "    # Phase 4 : Calibration\n",
        "\n",
        "\n",
        "\n",
        "    model_phase4 = ImprovedBertLSTMModel(audio_hidden=64, video_hidden=64,\n",
        "                                         lstm_layers=2, text_hidden=384).to(DEVICE)\n",
        "\n",
        "    param_groups_p4 = [\n",
        "        {'params': model_phase4.text_encoder.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase4.audio_encoder.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase4.video_encoder.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase4.audio_attention.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase4.video_attention.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase4.text_small_proj.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase4.gated_fusion.parameters(), 'lr': 1e-3},\n",
        "        {'params': model_phase4.classifier.parameters(), 'lr': 1e-3}\n",
        "    ]\n",
        "\n",
        "    optimizer_p4 = optim.AdamW(param_groups_p4, weight_decay=0.01)\n",
        "    scheduler_p4 = optim.lr_scheduler.CosineAnnealingLR(optimizer_p4,\n",
        "                                                        T_max=NUM_EPOCHS,\n",
        "                                                        eta_min=1e-6)\n",
        "\n",
        "    best_f1_p4 = 0\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model_phase4.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            word_indices = batch['word_indices'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            audio = batch['audio'].to(DEVICE)\n",
        "            video = batch['video'].to(DEVICE)\n",
        "            audio_lengths = batch['audio_lengths'].to(DEVICE)\n",
        "            video_lengths = batch['video_lengths'].to(DEVICE)\n",
        "            labels = batch['label'].to(DEVICE)\n",
        "\n",
        "            # Random modality dropout\n",
        "            if np.random.rand() < 0.15:\n",
        "                which = np.random.choice(['text', 'audio', 'video'])\n",
        "                if which == 'text':\n",
        "                    word_indices = torch.zeros_like(word_indices)\n",
        "                    attention_mask = torch.zeros_like(attention_mask)\n",
        "                elif which == 'audio':\n",
        "                    audio = torch.zeros_like(audio)\n",
        "                    audio_lengths = torch.ones_like(audio_lengths)\n",
        "                else:\n",
        "                    video = torch.zeros_like(video)\n",
        "                    video_lengths = torch.ones_like(video_lengths)\n",
        "\n",
        "            optimizer_p4.zero_grad()\n",
        "\n",
        "            logits = model_phase4(word_indices, audio, video,\n",
        "                                 audio_lengths, video_lengths,\n",
        "                                 attention_mask=attention_mask)\n",
        "            ce_loss = loss_fn(logits, labels)\n",
        "\n",
        "            # Calibration loss\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            max_prob = probs.max(dim=1)[0]\n",
        "            cal_loss = torch.clamp(max_prob - 0.8, min=0).mean()\n",
        "\n",
        "            loss = ce_loss + 0.2 * cal_loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model_phase4.parameters(), 1.0)\n",
        "            optimizer_p4.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        dev_acc, dev_f1 = evaluate(model_phase4, dev_loader, DEVICE)\n",
        "        scheduler_p4.step()\n",
        "\n",
        "        if dev_f1 > best_f1_p4:\n",
        "            best_f1_p4 = dev_f1\n",
        "            torch.save(model_phase4.state_dict(),\n",
        "                      f'{SAVE_FOLDER}/phase4_calibration.pt')\n",
        "            status = \" Saved\"\n",
        "        else:\n",
        "            status = \"\"\n",
        "\n",
        "        if (epoch + 1) % 3 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1:2d}/{NUM_EPOCHS} | Loss: {total_loss/len(train_loader):.4f} | \"\n",
        "                  f\"Dev Acc: {dev_acc:.4f} | F1: {dev_f1:.4f} | {status}\")\n",
        "\n",
        "    model_phase4.load_state_dict(torch.load(f'{SAVE_FOLDER}/phase4_calibration.pt'))\n",
        "    test_acc_p4, test_f1_p4 = evaluate(model_phase4, test_loader, DEVICE)\n",
        "    print(f\"\\n Phase 4 - Calibration: Acc={test_acc_p4:.4f}, F1={test_f1_p4:.4f}\\n\")\n",
        "\n",
        "\n",
        "    print(\"Phase 4 Missing Data:\")\n",
        "    for missing in [None, 'audio', 'video', 'text']:\n",
        "        acc, f1 = evaluate(model_phase4, test_loader, DEVICE, missing)\n",
        "        name = missing.upper() if missing else 'ALL'\n",
        "        print(f\"  Missing {name:6s}: Accuracy={acc:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "    # SUMMARY\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print(\" Result:\")\n",
        "    print(f\"Phase 1 (Baseline):      Acc={test_acc_p1:.4f}, F1={test_f1_p1:.4f}\")\n",
        "    print(f\"Phase 3 (KD):            Acc={test_acc_p3:.4f}, F1={test_f1_p3:.4f}\")\n",
        "    print(f\"Phase 4 (Calibration):   Acc={test_acc_p4:.4f}, F1={test_f1_p4:.4f}\")\n",
        "    print()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LufOjnNHYTQI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DATA_FOLDER = './ted_humor_data'\n",
        "SAVE_FOLDER = './saved_models'\n",
        "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 150\n",
        "LEARNING_RATE_TEXT = 5e-5\n",
        "LEARNING_RATE_LSTM = 5e-5\n",
        "LEARNING_RATE_MLP = 2e-4\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AUTO-ENSEMBLE: Training 3 models + ensemble\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "\n",
        "# Load embeddings once\n",
        "print(\"Loading embeddings\")\n",
        "with open(f'{DATA_FOLDER}/word_embedding_list.pkl', 'rb') as f:\n",
        "    word_embeddings_list = pickle.load(f, encoding='latin1')\n",
        "word_embeddings_array = np.array(word_embeddings_list, dtype=np.float32)\n",
        "embedding_dim = word_embeddings_array.shape[1]\n",
        "\n",
        "# Model architecture\n",
        "class SimpleTextEncoder(nn.Module):\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_tensor, freeze=False)\n",
        "        embed_dim = embedding_tensor.shape[1]\n",
        "        self.lstm = nn.LSTM(embed_dim, 128, num_layers=1, batch_first=True, bidirectional=True, dropout=0.0)\n",
        "        self.projection = nn.Linear(128, 256)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, word_indices):\n",
        "        if word_indices.shape[1] == 0:\n",
        "            return torch.zeros(word_indices.shape[0], 256).to(word_indices.device)\n",
        "        embeds = self.embedding(word_indices)\n",
        "        lstm_out, (h, c) = self.lstm(embeds)\n",
        "        out = h[-1]\n",
        "        out = self.projection(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class SimpleAudioEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(81, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 256),\n",
        "        )\n",
        "\n",
        "    def forward(self, audio_raw):\n",
        "        audio_pooled = audio_raw.mean(dim=1)\n",
        "        return self.net(audio_pooled)\n",
        "\n",
        "class SimpleVideoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(75, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 256),\n",
        "        )\n",
        "\n",
        "    def forward(self, video_raw):\n",
        "        video_pooled = video_raw.mean(dim=1)\n",
        "        return self.net(video_pooled)\n",
        "\n",
        "class SimpleFusion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(256 + 256 + 256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, text, audio, video):\n",
        "        combined = torch.cat([text, audio, video], dim=1)\n",
        "        return self.net(combined)\n",
        "\n",
        "class SimpleMultimodalModel(nn.Module):\n",
        "    def __init__(self, embedding_tensor):\n",
        "        super().__init__()\n",
        "        self.text_encoder = SimpleTextEncoder(embedding_tensor)\n",
        "        self.audio_encoder = SimpleAudioEncoder()\n",
        "        self.video_encoder = SimpleVideoEncoder()\n",
        "        self.fusion = SimpleFusion()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, word_indices, audio_raw, video_raw):\n",
        "        text_feat = self.text_encoder(word_indices)\n",
        "        audio_feat = self.audio_encoder(audio_raw)\n",
        "        video_feat = self.video_encoder(video_raw)\n",
        "        fused = self.fusion(text_feat, audio_feat, video_feat)\n",
        "        logits = self.classifier(fused)\n",
        "        return logits\n",
        "\n",
        "# Dataset\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, ids, text_data, audio_data, video_data, labels, audio_scaler, video_scaler):\n",
        "        self.ids = ids\n",
        "        self.text_data = text_data\n",
        "        self.audio_data = audio_data\n",
        "        self.video_data = video_data\n",
        "        self.labels = labels\n",
        "        self.audio_scaler = audio_scaler\n",
        "        self.video_scaler = video_scaler\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_id = self.ids[idx]\n",
        "\n",
        "        text_data = self.text_data[sample_id]\n",
        "        if isinstance(text_data, dict):\n",
        "            text_indices = text_data.get('punchline_features', text_data.get('punchline', list(text_data.values())[0]))\n",
        "        else:\n",
        "            text_indices = text_data\n",
        "\n",
        "        word_indices = []\n",
        "        if isinstance(text_indices, (list, np.ndarray)):\n",
        "            for idx_val in text_indices:\n",
        "                try:\n",
        "                    idx_val = int(idx_val[0]) if isinstance(idx_val, (list, np.ndarray)) and len(idx_val) > 0 else int(idx_val)\n",
        "                    if 0 <= idx_val < len(word_embeddings_list):\n",
        "                        word_indices.append(idx_val)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        if len(word_indices) == 0:\n",
        "            word_indices = [0]\n",
        "        word_indices = word_indices[:512]\n",
        "\n",
        "        audio_data = self.audio_data[sample_id]\n",
        "        if isinstance(audio_data, dict):\n",
        "            audio_features = audio_data.get('punchline_features', audio_data.get('punchline', list(audio_data.values())[0]))\n",
        "        else:\n",
        "            audio_features = audio_data\n",
        "\n",
        "        try:\n",
        "            audio_raw = np.array(audio_features, dtype=np.float32).reshape(-1, 81)\n",
        "            if audio_raw.shape[0] == 0:\n",
        "                audio_raw = np.zeros((1, 81), dtype=np.float32)\n",
        "            if self.audio_scaler:\n",
        "                audio_raw = self.audio_scaler.transform(audio_raw)\n",
        "        except:\n",
        "            audio_raw = np.zeros((1, 81), dtype=np.float32)\n",
        "\n",
        "        video_data = self.video_data[sample_id]\n",
        "        if isinstance(video_data, dict):\n",
        "            video_features = video_data.get('punchline_features', video_data.get('punchline', list(video_data.values())[0]))\n",
        "        else:\n",
        "            video_features = video_data\n",
        "\n",
        "        try:\n",
        "            video_raw = np.array(video_features, dtype=np.float32).reshape(-1, 75)\n",
        "            if video_raw.shape[0] == 0:\n",
        "                video_raw = np.zeros((1, 75), dtype=np.float32)\n",
        "            if self.video_scaler:\n",
        "                video_raw = self.video_scaler.transform(video_raw)\n",
        "        except:\n",
        "            video_raw = np.zeros((1, 75), dtype=np.float32)\n",
        "\n",
        "        return {\n",
        "            'word_indices': torch.LongTensor(word_indices),\n",
        "            'audio_raw': torch.FloatTensor(audio_raw),\n",
        "            'video_raw': torch.FloatTensor(video_raw),\n",
        "            'label': torch.tensor(self.labels[sample_id], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    max_text_len = max(len(item['word_indices']) for item in batch)\n",
        "    word_indices_padded = []\n",
        "    for item in batch:\n",
        "        indices = item['word_indices']\n",
        "        padded = np.pad(indices.numpy(), (0, max_text_len - len(indices)), constant_values=0)\n",
        "        word_indices_padded.append(torch.LongTensor(padded))\n",
        "\n",
        "    max_audio_len = max(item['audio_raw'].shape[0] for item in batch)\n",
        "    audio_raw_padded = []\n",
        "    for item in batch:\n",
        "        audio_raw = item['audio_raw'].numpy()\n",
        "        if audio_raw.shape[0] < max_audio_len:\n",
        "            padded = np.pad(audio_raw, ((0, max_audio_len - audio_raw.shape[0]), (0, 0)))\n",
        "        else:\n",
        "            padded = audio_raw\n",
        "        audio_raw_padded.append(torch.FloatTensor(padded))\n",
        "\n",
        "    max_video_len = max(item['video_raw'].shape[0] for item in batch)\n",
        "    video_raw_padded = []\n",
        "    for item in batch:\n",
        "        video_raw = item['video_raw'].numpy()\n",
        "        if video_raw.shape[0] < max_video_len:\n",
        "            padded = np.pad(video_raw, ((0, max_video_len - video_raw.shape[0]), (0, 0)))\n",
        "        else:\n",
        "            padded = video_raw\n",
        "        video_raw_padded.append(torch.FloatTensor(padded))\n",
        "\n",
        "    return {\n",
        "        'word_indices': torch.stack(word_indices_padded),\n",
        "        'audio_raw': torch.stack(audio_raw_padded),\n",
        "        'video_raw': torch.stack(video_raw_padded),\n",
        "        'label': torch.stack([item['label'] for item in batch])\n",
        "    }\n",
        "\n",
        "def train_epoch(model, loader, optimizer, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in loader:\n",
        "        word_indices = batch['word_indices'].to(device)\n",
        "        audio_raw = batch['audio_raw'].to(device)\n",
        "        video_raw = batch['video_raw'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(word_indices, audio_raw, video_raw)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            word_indices = batch['word_indices'].to(device)\n",
        "            audio_raw = batch['audio_raw'].to(device)\n",
        "            video_raw = batch['video_raw'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits = model(word_indices, audio_raw, video_raw)\n",
        "            pred = logits.argmax(dim=1)\n",
        "\n",
        "            preds.extend(pred.cpu().numpy())\n",
        "            targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(targets, preds)\n",
        "    f1 = f1_score(targets, preds, average='macro', zero_division=0)\n",
        "    return acc, f1\n",
        "\n",
        "# Load data once\n",
        "print(\"Loading data\")\n",
        "with open(f'{DATA_FOLDER}/data_folds.pkl', 'rb') as f:\n",
        "    data_folds = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/word_embedding_indexes_sdk.pkl', 'rb') as f:\n",
        "    text_features = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/covarep_features_sdk.pkl', 'rb') as f:\n",
        "    audio_features = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/openface_features_sdk.pkl', 'rb') as f:\n",
        "    video_features = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/humor_label_sdk.pkl', 'rb') as f:\n",
        "    labels = pickle.load(f, encoding='latin1')\n",
        "\n",
        "train_ids = data_folds['train']\n",
        "dev_ids = data_folds['dev']\n",
        "test_ids = data_folds['test']\n",
        "\n",
        "# Create scalers\n",
        "train_audio_list = []\n",
        "for id in train_ids:\n",
        "    try:\n",
        "        audio_data = audio_features.get(id, {})\n",
        "        if isinstance(audio_data, dict):\n",
        "            audio_feat = audio_data.get('punchline_features', audio_data.get('punchline', list(audio_data.values())[0] if audio_data else None))\n",
        "        else:\n",
        "            audio_feat = audio_data\n",
        "        if audio_feat is not None:\n",
        "            arr = np.array(audio_feat, dtype=np.float32).reshape(-1, 81)\n",
        "            if arr.shape[0] > 0:\n",
        "                train_audio_list.append(arr)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "scaler_audio = StandardScaler().fit(np.vstack(train_audio_list) if train_audio_list else np.zeros((1, 81)))\n",
        "\n",
        "train_video_list = []\n",
        "for id in train_ids:\n",
        "    try:\n",
        "        video_data = video_features.get(id, {})\n",
        "        if isinstance(video_data, dict):\n",
        "            video_feat = video_data.get('punchline_features', video_data.get('punchline', list(video_data.values())[0] if video_data else None))\n",
        "        else:\n",
        "            video_feat = video_data\n",
        "        if video_feat is not None:\n",
        "            arr = np.array(video_feat, dtype=np.float32).reshape(-1, 75)\n",
        "            if arr.shape[0] > 0:\n",
        "                train_video_list.append(arr)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "scaler_video = StandardScaler().fit(np.vstack(train_video_list) if train_video_list else np.zeros((1, 75)))\n",
        "\n",
        "# Train 3 models\n",
        "seeds = [42, 43, 44]\n",
        "trained_models = []\n",
        "\n",
        "for seed_idx, seed in enumerate(seeds):\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"TRAINING MODEL {seed_idx + 1}/3 (SEED={seed})\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Set random seeds\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Create datasets\n",
        "    train_set = SimpleDataset(train_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video)\n",
        "    dev_set = SimpleDataset(dev_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video)\n",
        "    test_set = SimpleDataset(test_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
        "    dev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "    # Loss with class weighting\n",
        "    train_labels = np.array([labels[id] for id in train_ids])\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "    class_weights = torch.FloatTensor(class_weights).to(DEVICE)\n",
        "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    # Model\n",
        "    embedding_tensor = torch.FloatTensor(word_embeddings_array).to(DEVICE)\n",
        "    model = SimpleMultimodalModel(embedding_tensor).to(DEVICE)\n",
        "\n",
        "    param_groups = [\n",
        "        {'params': model.text_encoder.parameters(), 'lr': LEARNING_RATE_TEXT},\n",
        "        {'params': model.audio_encoder.parameters(), 'lr': LEARNING_RATE_LSTM},\n",
        "        {'params': model.video_encoder.parameters(), 'lr': LEARNING_RATE_LSTM},\n",
        "        {'params': model.fusion.parameters(), 'lr': LEARNING_RATE_LSTM},\n",
        "        {'params': model.classifier.parameters(), 'lr': LEARNING_RATE_MLP}\n",
        "    ]\n",
        "\n",
        "    optimizer = optim.AdamW(param_groups, weight_decay=1e-5)\n",
        "    total_steps = NUM_EPOCHS * len(train_loader)\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=total_steps, num_cycles=0.5)\n",
        "\n",
        "    best_f1 = 0\n",
        "    patience = 20\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        loss = train_epoch(model, train_loader, optimizer, loss_fn, DEVICE)\n",
        "        dev_acc, dev_f1 = evaluate(model, dev_loader, DEVICE)\n",
        "        scheduler.step()\n",
        "\n",
        "        if dev_f1 > best_f1:\n",
        "            best_f1 = dev_f1\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), f'{SAVE_FOLDER}/simple_best_seed{seed}.pt')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1:3d} | Loss: {loss:.4f} | Dev F1: {dev_f1:.4f}\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopped at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    # Evaluate on test\n",
        "    model.load_state_dict(torch.load(f'{SAVE_FOLDER}/simple_best_seed{seed}.pt'))\n",
        "    test_acc, test_f1 = evaluate(model, test_loader, DEVICE)\n",
        "    print(f\"\\n Model {seed_idx + 1} test accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "\n",
        "    trained_models.append((model, test_acc, test_f1))\n",
        "\n",
        "# Ensemble evaluation\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ENSEMBLE EVALUATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_set = SimpleDataset(test_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "all_preds_list = []\n",
        "all_labels_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        word_indices = batch['word_indices'].to(DEVICE)\n",
        "        audio_raw = batch['audio_raw'].to(DEVICE)\n",
        "        video_raw = batch['video_raw'].to(DEVICE)\n",
        "        labels_batch = batch['label'].to(DEVICE)\n",
        "\n",
        "        logits_list = []\n",
        "        for model, _, _ in trained_models:\n",
        "            model.eval()\n",
        "            logits = model(word_indices, audio_raw, video_raw)\n",
        "            logits_list.append(logits)\n",
        "\n",
        "        avg_logits = sum(logits_list) / len(logits_list)\n",
        "        preds = avg_logits.argmax(dim=1)\n",
        "\n",
        "        all_preds_list.append(preds)\n",
        "        all_labels_list.append(labels_batch)\n",
        "\n",
        "all_preds = torch.cat(all_preds_list).cpu().numpy()\n",
        "all_labels = torch.cat(all_labels_list).cpu().numpy()\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "prec = precision_score(all_labels, all_preds, average='macro')\n",
        "rec = recall_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ENSEMBLE RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nIndividual models:\")\n",
        "for i, (_, test_acc, test_f1) in enumerate(trained_models, 1):\n",
        "    print(f\"  Model {i}: Accuracy={test_acc:.4f}, F1={test_f1:.4f}\")\n",
        "\n",
        "print(f\"\\n ENSEMBLE RESULTS:\")\n",
        "print(f\"  Accuracy:  {acc:.4f} ({acc*100:.2f}%)\")\n",
        "print(f\"  F1 Score:  {f1:.4f}\")\n",
        "print(f\"  Precision: {prec:.4f}\")\n",
        "print(f\"  Recall:    {rec:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Overall\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\n1. 3 models trained: simple_best_seed42.pt, simple_best_seed43.pt, simple_best_seed44.pt\")\n",
        "print(\"2.  Ensemble accuracy:\", f\"{acc*100:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF5hRC8iZW9L"
      },
      "outputs": [],
      "source": [
        "Loading embeddings...\n",
        "Loading data...\n",
        "\n",
        "================================================================================\n",
        "TRAINING MODEL 1/3 (SEED=42)\n",
        "================================================================================\n",
        "Epoch  10 | Loss: 0.6934 | Dev F1: 0.3333\n",
        "Epoch  20 | Loss: 0.6909 | Dev F1: 0.5174\n",
        "Epoch  30 | Loss: 0.6675 | Dev F1: 0.5826\n",
        "Epoch  40 | Loss: 0.6289 | Dev F1: 0.6226\n",
        "Epoch  50 | Loss: 0.6020 | Dev F1: 0.6529\n",
        "Epoch  60 | Loss: 0.5846 | Dev F1: 0.6647\n",
        "Epoch  70 | Loss: 0.5656 | Dev F1: 0.6716\n",
        "Epoch  80 | Loss: 0.5412 | Dev F1: 0.6795\n",
        "Epoch  90 | Loss: 0.5175 | Dev F1: 0.6799\n",
        "Epoch 100 | Loss: 0.4884 | Dev F1: 0.6742\n",
        "Early stopped at epoch 104\n",
        "\n",
        " Model 1 test accuracy: 0.6805 (68.05%)\n",
        "\n",
        "================================================================================\n",
        "TRAINING MODEL 2/3 (SEED=43)\n",
        "================================================================================\n",
        "Epoch  10 | Loss: 0.6935 | Dev F1: 0.3333\n",
        "Epoch  20 | Loss: 0.6905 | Dev F1: 0.5072\n",
        "Epoch  30 | Loss: 0.6697 | Dev F1: 0.5650\n",
        "Epoch  40 | Loss: 0.6357 | Dev F1: 0.6186\n",
        "Epoch  50 | Loss: 0.6039 | Dev F1: 0.6412\n",
        "Epoch  60 | Loss: 0.5860 | Dev F1: 0.6603\n",
        "Epoch  70 | Loss: 0.5683 | Dev F1: 0.6691\n",
        "Epoch  80 | Loss: 0.5458 | Dev F1: 0.6781\n",
        "Epoch  90 | Loss: 0.5177 | Dev F1: 0.6792\n",
        "Epoch 100 | Loss: 0.4841 | Dev F1: 0.6795\n",
        "Epoch 110 | Loss: 0.4510 | Dev F1: 0.6662\n",
        "Early stopped at epoch 117\n",
        "\n",
        " Model 2 test accuracy: 0.6900 (69.00%)\n",
        "\n",
        "================================================================================\n",
        "TRAINING MODEL 3/3 (SEED=44)\n",
        "================================================================================\n",
        "Epoch  10 | Loss: 0.6946 | Dev F1: 0.3333\n",
        "Epoch  20 | Loss: 0.6920 | Dev F1: 0.3535\n",
        "Epoch  30 | Loss: 0.6734 | Dev F1: 0.5614\n",
        "Epoch  40 | Loss: 0.6384 | Dev F1: 0.6069\n",
        "Epoch  50 | Loss: 0.6078 | Dev F1: 0.6395\n",
        "Epoch  60 | Loss: 0.5880 | Dev F1: 0.6596\n",
        "Epoch  70 | Loss: 0.5685 | Dev F1: 0.6713\n",
        "Epoch  80 | Loss: 0.5460 | Dev F1: 0.6773\n",
        "Epoch  90 | Loss: 0.5172 | Dev F1: 0.6728\n",
        "Epoch 100 | Loss: 0.4860 | Dev F1: 0.6607\n",
        "Epoch 110 | Loss: 0.4473 | Dev F1: 0.6568\n",
        "Early stopped at epoch 111\n",
        "\n",
        " Model 3 test accuracy: 0.6836 (68.36%)\n",
        "\n",
        "================================================================================\n",
        "ENSEMBLE EVALUATION\n",
        "================================================================================\n",
        "\n",
        "================================================================================\n",
        "ENSEMBLE RESULTS\n",
        "================================================================================\n",
        "\n",
        "Individual models:\n",
        "  Model 1: Accuracy=0.6805, F1=0.6785\n",
        "  Model 2: Accuracy=0.6900, F1=0.6899\n",
        "  Model 3: Accuracy=0.6836, F1=0.6833\n",
        "\n",
        "ENSEMBLE RESULTS:\n",
        "  Accuracy:  0.6988 (69.88%)\n",
        "  F1 Score:  0.6984\n",
        "  Precision: 0.7002\n",
        "  Recall:    0.6990\n",
        "\n",
        "\n",
        "\n",
        "================================================================================\n",
        "Overall:\n",
        "================================================================================\n",
        "\n",
        "1.  3 models trained: simple_best_seed42.pt, simple_best_seed43.pt, simple_best_seed44.pt\n",
        "2.  Ensemble accuracy: 69.88%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ETNFjq0xbV-0"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DATA_FOLDER = './ted_humor_data'\n",
        "SAVE_FOLDER = './saved_models_phase'\n",
        "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ROBUST MULTIMODAL FUSION WITH MISSING MODALITIES\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"Device: {DEVICE}\\n\")\n",
        "\n",
        "# Load all data\n",
        "print(\"Loading data...\")\n",
        "with open(f'{DATA_FOLDER}/data_folds.pkl', 'rb') as f:\n",
        "    data_folds = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/word_embedding_indexes_sdk.pkl', 'rb') as f:\n",
        "    text_features = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/covarep_features_sdk.pkl', 'rb') as f:\n",
        "    audio_features = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/openface_features_sdk.pkl', 'rb') as f:\n",
        "    video_features = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/humor_label_sdk.pkl', 'rb') as f:\n",
        "    labels = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/word_embedding_list.pkl', 'rb') as f:\n",
        "    word_embeddings_list = pickle.load(f, encoding='latin1')\n",
        "\n",
        "train_ids = data_folds['train']\n",
        "dev_ids = data_folds['dev']\n",
        "test_ids = data_folds['test']\n",
        "\n",
        "word_embeddings_array = np.array(word_embeddings_list, dtype=np.float32)\n",
        "\n",
        "# Create scalers\n",
        "train_audio_list = []\n",
        "for id in train_ids:\n",
        "    try:\n",
        "        audio_data = audio_features.get(id, {})\n",
        "        if isinstance(audio_data, dict):\n",
        "            audio_feat = audio_data.get('punchline_features', audio_data.get('punchline', list(audio_data.values())[0] if audio_data else None))\n",
        "        else:\n",
        "            audio_feat = audio_data\n",
        "        if audio_feat is not None:\n",
        "            arr = np.array(audio_feat, dtype=np.float32).reshape(-1, 81)\n",
        "            if arr.shape[0] > 0:\n",
        "                train_audio_list.append(arr)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "scaler_audio = StandardScaler().fit(np.vstack(train_audio_list) if train_audio_list else np.zeros((1, 81)))\n",
        "\n",
        "train_video_list = []\n",
        "for id in train_ids:\n",
        "    try:\n",
        "        video_data = video_features.get(id, {})\n",
        "        if isinstance(video_data, dict):\n",
        "            video_feat = video_data.get('punchline_features', video_data.get('punchline', list(video_data.values())[0] if video_data else None))\n",
        "        else:\n",
        "            video_feat = video_data\n",
        "        if video_feat is not None:\n",
        "            arr = np.array(video_feat, dtype=np.float32).reshape(-1, 75)\n",
        "            if arr.shape[0] > 0:\n",
        "                train_video_list.append(arr)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "scaler_video = StandardScaler().fit(np.vstack(train_video_list) if train_video_list else np.zeros((1, 75)))\n",
        "\n",
        "# Cross-Modal Attention Mechanism\n",
        "\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, dim=256):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "        self.query = nn.Linear(dim, dim)\n",
        "        self.key = nn.Linear(dim, dim)\n",
        "        self.value = nn.Linear(dim, dim)\n",
        "        self.out_proj = nn.Linear(dim, dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, text_feat, modality_feat):\n",
        "        q = self.query(text_feat)\n",
        "        k = self.key(modality_feat)\n",
        "        v = self.value(modality_feat)\n",
        "\n",
        "        scores = torch.matmul(q.unsqueeze(1), k.unsqueeze(2)) / (self.dim ** 0.5)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        attn_output = torch.matmul(attn_weights, v.unsqueeze(1)).squeeze(1)\n",
        "        output = self.out_proj(attn_output)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Improved Multimodal Fusion\n",
        "\n",
        "class ImprovedMultimodalFusion(nn.Module):\n",
        "\n",
        "    def __init__(self, word_embeddings_array):\n",
        "        super().__init__()\n",
        "\n",
        "        # Text: Embedding + Bidirectional LSTM\n",
        "        embedding_tensor = torch.FloatTensor(word_embeddings_array)\n",
        "        self.text_embedding = nn.Embedding.from_pretrained(embedding_tensor, freeze=False)\n",
        "        self.text_lstm = nn.LSTM(300, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Audio: Bidirectional LSTM on raw 81-dim frames\n",
        "        self.audio_lstm = nn.LSTM(81, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Video: Bidirectional LSTM on raw 75-dim frames\n",
        "        self.video_lstm = nn.LSTM(75, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Cross-modal attention mechanisms\n",
        "        self.audio_attention = CrossModalAttention(dim=256)\n",
        "        self.video_attention = CrossModalAttention(dim=256)\n",
        "\n",
        "        # Fusion layer\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def extract_final_hidden_state(self, lstm_output):\n",
        "        \"\"\"Extract final hidden state from bidirectional LSTM\"\"\"\n",
        "        forward_final = lstm_output[:, -1, :128]\n",
        "        backward_final = lstm_output[:, 0, 128:]\n",
        "        final_state = torch.cat([forward_final, backward_final], dim=1)\n",
        "        return final_state\n",
        "\n",
        "    def forward(self, word_indices, audio_raw, video_raw, missing_mask=None):\n",
        "        # Text\n",
        "        text_embeds = self.text_embedding(word_indices)\n",
        "        text_lstm_output, _ = self.text_lstm(text_embeds)\n",
        "        text_feat = self.extract_final_hidden_state(text_lstm_output)\n",
        "\n",
        "        # Audio\n",
        "        audio_lstm_output, _ = self.audio_lstm(audio_raw)\n",
        "        audio_feat = self.extract_final_hidden_state(audio_lstm_output)\n",
        "\n",
        "        # Video\n",
        "        video_lstm_output, _ = self.video_lstm(video_raw)\n",
        "        video_feat = self.extract_final_hidden_state(video_lstm_output)\n",
        "\n",
        "        # Apply missing masks\n",
        "        if missing_mask is not None:\n",
        "            if missing_mask.get('text', False):\n",
        "                text_feat = torch.zeros_like(text_feat)\n",
        "            if missing_mask.get('audio', False):\n",
        "                audio_feat = torch.zeros_like(audio_feat)\n",
        "            if missing_mask.get('video', False):\n",
        "                video_feat = torch.zeros_like(video_feat)\n",
        "\n",
        "        # Cross-modal attention: Align audio/video to text\n",
        "        audio_aligned = self.audio_attention(text_feat, audio_feat)\n",
        "        video_aligned = self.video_attention(text_feat, video_feat)\n",
        "\n",
        "        # Fusion: Concatenate aligned features\n",
        "        fused_input = torch.cat([text_feat, audio_aligned, video_aligned], dim=1)\n",
        "        fused = self.fusion(fused_input)\n",
        "        logits = self.classifier(fused)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Dataset & Collate\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, ids, text_data, audio_data, video_data, labels, audio_scaler, video_scaler, word_embeddings_list, modality_drop_rate=0.0):\n",
        "        self.ids = ids\n",
        "        self.text_data = text_data\n",
        "        self.audio_data = audio_data\n",
        "        self.video_data = video_data\n",
        "        self.labels = labels\n",
        "        self.audio_scaler = audio_scaler\n",
        "        self.video_scaler = video_scaler\n",
        "        self.word_embeddings_list = word_embeddings_list\n",
        "        self.modality_drop_rate = modality_drop_rate\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_id = self.ids[idx]\n",
        "\n",
        "        # Text\n",
        "        text_data = self.text_data[sample_id]\n",
        "        if isinstance(text_data, dict):\n",
        "            text_indices = text_data.get('punchline_features', text_data.get('punchline', list(text_data.values())[0]))\n",
        "        else:\n",
        "            text_indices = text_data\n",
        "\n",
        "        word_indices = []\n",
        "        if isinstance(text_indices, (list, np.ndarray)):\n",
        "            for idx_val in text_indices:\n",
        "                try:\n",
        "                    idx_val = int(idx_val[0]) if isinstance(idx_val, (list, np.ndarray)) and len(idx_val) > 0 else int(idx_val)\n",
        "                    if 0 <= idx_val < len(self.word_embeddings_list):\n",
        "                        word_indices.append(idx_val)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        if len(word_indices) == 0:\n",
        "            word_indices = [0]\n",
        "        word_indices = word_indices[:512]\n",
        "\n",
        "        # Audio\n",
        "        audio_data = self.audio_data[sample_id]\n",
        "        if isinstance(audio_data, dict):\n",
        "            audio_features = audio_data.get('punchline_features', audio_data.get('punchline', list(audio_data.values())[0]))\n",
        "        else:\n",
        "            audio_features = audio_data\n",
        "\n",
        "        try:\n",
        "            audio_raw = np.array(audio_features, dtype=np.float32).reshape(-1, 81)\n",
        "            if audio_raw.shape[0] == 0:\n",
        "                audio_raw = np.zeros((1, 81), dtype=np.float32)\n",
        "            if self.audio_scaler:\n",
        "                audio_raw = self.audio_scaler.transform(audio_raw)\n",
        "        except:\n",
        "            audio_raw = np.zeros((1, 81), dtype=np.float32)\n",
        "\n",
        "        # Video\n",
        "        video_data = self.video_data[sample_id]\n",
        "        if isinstance(video_data, dict):\n",
        "            video_features = video_data.get('punchline_features', video_data.get('punchline', list(video_data.values())[0]))\n",
        "        else:\n",
        "            video_features = video_data\n",
        "\n",
        "        try:\n",
        "            video_raw = np.array(video_features, dtype=np.float32).reshape(-1, 75)\n",
        "            if video_raw.shape[0] == 0:\n",
        "                video_raw = np.zeros((1, 75), dtype=np.float32)\n",
        "            if self.video_scaler:\n",
        "                video_raw = self.video_scaler.transform(video_raw)\n",
        "        except:\n",
        "            video_raw = np.zeros((1, 75), dtype=np.float32)\n",
        "\n",
        "        missing_mask = {\n",
        "            'text': np.random.rand() < self.modality_drop_rate,\n",
        "            'audio': np.random.rand() < self.modality_drop_rate,\n",
        "            'video': np.random.rand() < self.modality_drop_rate\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            'word_indices': torch.LongTensor(word_indices),\n",
        "            'audio_raw': torch.FloatTensor(audio_raw),\n",
        "            'video_raw': torch.FloatTensor(video_raw),\n",
        "            'label': torch.tensor(self.labels[sample_id], dtype=torch.long),\n",
        "            'missing_mask': missing_mask\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    max_text_len = max(len(item['word_indices']) for item in batch)\n",
        "    word_indices_padded = [torch.LongTensor(np.pad(item['word_indices'].numpy(), (0, max_text_len - len(item['word_indices'])), constant_values=0)) for item in batch]\n",
        "\n",
        "    max_audio_len = max(item['audio_raw'].shape[0] for item in batch)\n",
        "    audio_raw_padded = []\n",
        "    for item in batch:\n",
        "        audio_raw = item['audio_raw'].numpy()\n",
        "        if audio_raw.shape[0] < max_audio_len:\n",
        "            padded = np.pad(audio_raw, ((0, max_audio_len - audio_raw.shape[0]), (0, 0)))\n",
        "        else:\n",
        "            padded = audio_raw\n",
        "        audio_raw_padded.append(torch.FloatTensor(padded))\n",
        "\n",
        "    max_video_len = max(item['video_raw'].shape[0] for item in batch)\n",
        "    video_raw_padded = []\n",
        "    for item in batch:\n",
        "        video_raw = item['video_raw'].numpy()\n",
        "        if video_raw.shape[0] < max_video_len:\n",
        "            padded = np.pad(video_raw, ((0, max_video_len - video_raw.shape[0]), (0, 0)))\n",
        "        else:\n",
        "            padded = video_raw\n",
        "        video_raw_padded.append(torch.FloatTensor(padded))\n",
        "\n",
        "    return {\n",
        "        'word_indices': torch.stack(word_indices_padded),\n",
        "        'audio_raw': torch.stack(audio_raw_padded),\n",
        "        'video_raw': torch.stack(video_raw_padded),\n",
        "        'label': torch.stack([item['label'] for item in batch]),\n",
        "        'missing_mask': [item['missing_mask'] for item in batch]\n",
        "    }\n",
        "\n",
        "# Training and Evaluation\n",
        "\n",
        "def train_epoch(model, loader, optimizer, loss_fn, device, use_kd=False, teacher=None, temperature=1.0, lambda_kd=0.5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in loader:\n",
        "        word_indices = batch['word_indices'].to(device)\n",
        "        audio_raw = batch['audio_raw'].to(device)\n",
        "        video_raw = batch['video_raw'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(word_indices, audio_raw, video_raw, missing_mask=None)\n",
        "        loss_ce = loss_fn(logits, labels)\n",
        "\n",
        "        if use_kd and teacher is not None:\n",
        "            teacher.eval()\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher(word_indices, audio_raw, video_raw, missing_mask=None)\n",
        "\n",
        "            student_probs = F.log_softmax(logits / temperature, dim=1)\n",
        "            teacher_probs = F.softmax(teacher_logits / temperature, dim=1)\n",
        "            loss_kd = F.kl_div(student_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
        "\n",
        "            loss = loss_ce + lambda_kd * loss_kd\n",
        "        else:\n",
        "            loss = loss_ce\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, device, compute_ece=False):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets = []\n",
        "    confidences = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            word_indices = batch['word_indices'].to(device)\n",
        "            audio_raw = batch['audio_raw'].to(device)\n",
        "            video_raw = batch['video_raw'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits = model(word_indices, audio_raw, video_raw, missing_mask=None)\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            conf = probs.max(dim=1)[0]\n",
        "\n",
        "            preds.extend(pred.cpu().numpy())\n",
        "            targets.extend(labels.cpu().numpy())\n",
        "            confidences.extend(conf.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(targets, preds)\n",
        "    f1 = f1_score(targets, preds, average='macro', zero_division=0)\n",
        "    prec = precision_score(targets, preds, average='macro', zero_division=0)\n",
        "    rec = recall_score(targets, preds, average='macro', zero_division=0)\n",
        "\n",
        "    ece = 0.0\n",
        "    if compute_ece:\n",
        "        confidences = np.array(confidences)\n",
        "        n_bins = 10\n",
        "        bin_edges = np.linspace(0, 1, n_bins + 1)\n",
        "        preds_array = np.array(preds)\n",
        "        targets_array = np.array(targets)\n",
        "\n",
        "        for i in range(n_bins):\n",
        "            mask = (confidences >= bin_edges[i]) & (confidences < bin_edges[i+1])\n",
        "            if mask.sum() > 0:\n",
        "                bin_acc = (preds_array[mask] == targets_array[mask]).mean()\n",
        "                bin_conf = confidences[mask].mean()\n",
        "                ece += mask.sum() / len(preds) * abs(bin_acc - bin_conf)\n",
        "\n",
        "    return acc, f1, prec, rec, ece\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE 1: Working Baseline\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\n Enhanced Architecture:\")\n",
        "\n",
        "\n",
        "baseline_results = {}\n",
        "\n",
        "# Train unimodal models for comparison\n",
        "class TextOnlyModel(nn.Module):\n",
        "    def __init__(self, word_embeddings_array):\n",
        "        super().__init__()\n",
        "        embedding_tensor = torch.FloatTensor(word_embeddings_array)\n",
        "        self.text_embedding = nn.Embedding.from_pretrained(embedding_tensor, freeze=False)\n",
        "        self.text_lstm = nn.LSTM(300, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, word_indices, audio_raw=None, video_raw=None, missing_mask=None):\n",
        "        text_embeds = self.text_embedding(word_indices)\n",
        "        text_lstm_output, _ = self.text_lstm(text_embeds)\n",
        "        forward_final = text_lstm_output[:, -1, :128]\n",
        "        backward_final = text_lstm_output[:, 0, 128:]\n",
        "        text_feat = torch.cat([forward_final, backward_final], dim=1)\n",
        "        return self.classifier(text_feat)\n",
        "\n",
        "class AudioOnlyModel(nn.Module):\n",
        "    def __init__(self, word_embeddings_array):\n",
        "        super().__init__()\n",
        "        self.audio_lstm = nn.LSTM(81, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, word_indices=None, audio_raw=None, video_raw=None, missing_mask=None):\n",
        "        audio_lstm_output, _ = self.audio_lstm(audio_raw)\n",
        "        forward_final = audio_lstm_output[:, -1, :128]\n",
        "        backward_final = audio_lstm_output[:, 0, 128:]\n",
        "        audio_feat = torch.cat([forward_final, backward_final], dim=1)\n",
        "        return self.classifier(audio_feat)\n",
        "\n",
        "class VideoOnlyModel(nn.Module):\n",
        "    def __init__(self, word_embeddings_array):\n",
        "        super().__init__()\n",
        "        self.video_lstm = nn.LSTM(75, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, word_indices=None, audio_raw=None, video_raw=None, missing_mask=None):\n",
        "        video_lstm_output, _ = self.video_lstm(video_raw)\n",
        "        forward_final = video_lstm_output[:, -1, :128]\n",
        "        backward_final = video_lstm_output[:, 0, 128:]\n",
        "        video_feat = torch.cat([forward_final, backward_final], dim=1)\n",
        "        return self.classifier(video_feat)\n",
        "\n",
        "teachers = {}\n",
        "\n",
        "for teacher_name, ModelClass in [('TEXT_teacher', TextOnlyModel), ('AUDIO_teacher', AudioOnlyModel), ('VIDEO_teacher', VideoOnlyModel)]:\n",
        "    print(f\"\\nTraining {teacher_name}\")\n",
        "\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    train_set = SimpleDataset(train_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, 0.0)\n",
        "    dev_set = SimpleDataset(dev_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, 0.0)\n",
        "    test_set = SimpleDataset(test_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, 0.0)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
        "    dev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "    train_labels_array = np.array([labels[id] for id in train_ids])\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels_array), y=train_labels_array)\n",
        "    class_weights = torch.FloatTensor(class_weights).to(DEVICE)\n",
        "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    model = ModelClass(word_embeddings_array).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=NUM_EPOCHS * len(train_loader), num_cycles=0.5)\n",
        "\n",
        "    best_f1 = 0\n",
        "    patience = 15\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        loss = train_epoch(model, train_loader, optimizer, loss_fn, DEVICE)\n",
        "        dev_acc, dev_f1, _, _ = evaluate(model, dev_loader, DEVICE)[:4]\n",
        "        scheduler.step()\n",
        "\n",
        "        if dev_f1 > best_f1:\n",
        "            best_f1 = dev_f1\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), f'{SAVE_FOLDER}/{teacher_name}.pt')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f\"Epoch {epoch+1:3d} | Loss: {loss:.4f} | Dev F1: {dev_f1:.4f}\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopped at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    model.load_state_dict(torch.load(f'{SAVE_FOLDER}/{teacher_name}.pt'))\n",
        "    test_acc, test_f1, test_prec, test_rec, _ = evaluate(model, test_loader, DEVICE)\n",
        "\n",
        "    teachers[teacher_name] = model\n",
        "    baseline_results[teacher_name] = {'accuracy': test_acc, 'f1': test_f1, 'precision': test_prec, 'recall': test_rec}\n",
        "    print(f\"  {teacher_name}: {test_acc*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nTraining multimodal baseline (with cross-modal attention)\")\n",
        "\n",
        "train_set = SimpleDataset(train_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, 0.0)\n",
        "dev_set = SimpleDataset(dev_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, 0.0)\n",
        "test_set = SimpleDataset(test_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, 0.0)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
        "dev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "baseline_model = ImprovedMultimodalFusion(word_embeddings_array).to(DEVICE)\n",
        "optimizer = optim.AdamW(baseline_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=NUM_EPOCHS * len(train_loader), num_cycles=0.5)\n",
        "\n",
        "best_f1 = 0\n",
        "patience = 15\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    loss = train_epoch(baseline_model, train_loader, optimizer, loss_fn, DEVICE)\n",
        "    dev_acc, dev_f1, _, _ = evaluate(baseline_model, dev_loader, DEVICE)[:4]\n",
        "    scheduler.step()\n",
        "\n",
        "    if dev_f1 > best_f1:\n",
        "        best_f1 = dev_f1\n",
        "        patience_counter = 0\n",
        "        torch.save(baseline_model.state_dict(), f'{SAVE_FOLDER}/baseline_multimodal.pt')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1:3d} | Loss: {loss:.4f} | Dev F1: {dev_f1:.4f}\")\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Early stopped at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "baseline_model.load_state_dict(torch.load(f'{SAVE_FOLDER}/baseline_multimodal.pt'))\n",
        "test_acc, test_f1, test_prec, test_rec, ece = evaluate(baseline_model, test_loader, DEVICE, compute_ece=True)\n",
        "\n",
        "baseline_results['Multimodal_Baseline'] = {'accuracy': test_acc, 'f1': test_f1, 'precision': test_prec, 'recall': test_rec, 'ece': ece}\n",
        "\n",
        "print(f\"✅ Multimodal Baseline: {test_acc*100:.2f}%\\n\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE 2: Stress Test\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nEvaluating baseline under missing modalities\")\n",
        "\n",
        "test_set_no_drop = SimpleDataset(test_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, 0.0)\n",
        "test_loader_no_drop = DataLoader(test_set_no_drop, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "missingness_patterns = [\n",
        "    {'text': False, 'audio': False, 'video': False, 'name': 'All Present'},\n",
        "    {'text': True, 'audio': False, 'video': False, 'name': 'Text Missing'},\n",
        "    {'text': False, 'audio': True, 'video': False, 'name': 'Audio Missing'},\n",
        "    {'text': False, 'audio': False, 'video': True, 'name': 'Video Missing'},\n",
        "    {'text': True, 'audio': True, 'video': False, 'name': 'Text+Audio Missing'},\n",
        "    {'text': True, 'audio': False, 'video': True, 'name': 'Text+Video Missing'},\n",
        "    {'text': False, 'audio': True, 'video': True, 'name': 'Audio+Video Missing'},\n",
        "    {'text': True, 'audio': True, 'video': True, 'name': 'All Missing'},\n",
        "]\n",
        "\n",
        "print(f\"\\n{'Missingness Pattern':<30} {'Accuracy':<12} {'F1':<10} {'Drop':<10}\")\n",
        "print(\"-\" * 62)\n",
        "\n",
        "missingness_results = {}\n",
        "all_present_acc = None\n",
        "\n",
        "with torch.no_grad():\n",
        "    for pattern in missingness_patterns:\n",
        "        preds, targets = [], []\n",
        "\n",
        "        for batch in test_loader_no_drop:\n",
        "            word_indices = batch['word_indices'].to(DEVICE)\n",
        "            audio_raw = batch['audio_raw'].to(DEVICE)\n",
        "            video_raw = batch['video_raw'].to(DEVICE)\n",
        "            labels_batch = batch['label'].to(DEVICE)\n",
        "\n",
        "            missing_mask = {k: v for k, v in pattern.items() if k != 'name'}\n",
        "            logits = baseline_model(word_indices, audio_raw, video_raw, missing_mask=missing_mask)\n",
        "\n",
        "            preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "            targets.extend(labels_batch.cpu().numpy())\n",
        "\n",
        "        acc = accuracy_score(targets, preds)\n",
        "        f1 = f1_score(targets, preds, average='macro', zero_division=0)\n",
        "\n",
        "        if pattern['name'] == 'All Present':\n",
        "            all_present_acc = acc\n",
        "            drop = 0.0\n",
        "        else:\n",
        "            drop = (all_present_acc - acc) * 100\n",
        "\n",
        "        missingness_results[pattern['name']] = {'accuracy': acc, 'f1': f1, 'drop': drop}\n",
        "        print(f\"{pattern['name']:<30} {acc:.4f} ({acc*100:5.2f}%)  {f1:.4f}    {drop:+.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PHASE 3: Cross Modal Knowledge Distillation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "best_teacher_name = max([(k, v['accuracy']) for k, v in baseline_results.items() if 'teacher' in k], key=lambda x: x[1])[0]\n",
        "best_teacher = teachers[best_teacher_name]\n",
        "print(f\"\\nBest teacher: {best_teacher_name} ({baseline_results[best_teacher_name]['accuracy']*100:.2f}%)\\n\")\n",
        "\n",
        "train_set_kd = SimpleDataset(train_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, 0.0)\n",
        "dev_set_kd = SimpleDataset(dev_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, 0.0)\n",
        "test_set_kd = SimpleDataset(test_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, 0.0)\n",
        "\n",
        "train_loader_kd = DataLoader(train_set_kd, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
        "dev_loader_kd = DataLoader(dev_set_kd, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)\n",
        "test_loader_kd = DataLoader(test_set_kd, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "kd_student = ImprovedMultimodalFusion(word_embeddings_array).to(DEVICE)\n",
        "optimizer_kd = optim.AdamW(kd_student.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "scheduler_kd = get_cosine_schedule_with_warmup(optimizer_kd, num_warmup_steps=500, num_training_steps=NUM_EPOCHS * len(train_loader_kd), num_cycles=0.5)\n",
        "\n",
        "best_f1_kd = 0\n",
        "patience_kd = 15\n",
        "patience_counter_kd = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    loss = train_epoch(kd_student, train_loader_kd, optimizer_kd, loss_fn, DEVICE, use_kd=True, teacher=best_teacher, temperature=4.0, lambda_kd=0.5)\n",
        "    dev_acc, dev_f1, _, _ = evaluate(kd_student, dev_loader_kd, DEVICE)[:4]\n",
        "    scheduler_kd.step()\n",
        "\n",
        "    if dev_f1 > best_f1_kd:\n",
        "        best_f1_kd = dev_f1\n",
        "        patience_counter_kd = 0\n",
        "        torch.save(kd_student.state_dict(), f'{SAVE_FOLDER}/kd_student.pt')\n",
        "    else:\n",
        "        patience_counter_kd += 1\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1:3d} | Loss: {loss:.4f} | Dev F1: {dev_f1:.4f}\")\n",
        "\n",
        "    if patience_counter_kd >= patience_kd:\n",
        "        print(f\"Early stopped at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "kd_student.load_state_dict(torch.load(f'{SAVE_FOLDER}/kd_student.pt'))\n",
        "test_acc_kd, test_f1_kd, test_prec_kd, test_rec_kd, ece_kd = evaluate(kd_student, test_loader_kd, DEVICE, compute_ece=True)\n",
        "\n",
        "print(f\"\\n KD Student: {test_acc_kd*100:.2f}% (vs Baseline: {baseline_results['Multimodal_Baseline']['accuracy']*100:.2f}%)\")\n",
        "print(f\"   Improvement: {(test_acc_kd - baseline_results['Multimodal_Baseline']['accuracy'])*100:+.2f} pp\\n\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE 4: Confidence Calibration under missingness\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "train_set_cal = SimpleDataset(train_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, 0.2)\n",
        "dev_set_cal = SimpleDataset(dev_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, 0.0)\n",
        "test_set_cal = SimpleDataset(test_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, 0.0)\n",
        "\n",
        "train_loader_cal = DataLoader(train_set_cal, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
        "dev_loader_cal = DataLoader(dev_set_cal, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)\n",
        "test_loader_cal = DataLoader(test_set_cal, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "calibrated_model = ImprovedMultimodalFusion(word_embeddings_array).to(DEVICE)\n",
        "optimizer_cal = optim.AdamW(calibrated_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "scheduler_cal = get_cosine_schedule_with_warmup(optimizer_cal, num_warmup_steps=500, num_training_steps=NUM_EPOCHS * len(train_loader_cal), num_cycles=0.5)\n",
        "\n",
        "best_f1_cal = 0\n",
        "patience_cal = 15\n",
        "patience_counter_cal = 0\n",
        "\n",
        "print(\"\\nTraining with 20% modality dropout\\n\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    loss = train_epoch(calibrated_model, train_loader_cal, optimizer_cal, loss_fn, DEVICE)\n",
        "    dev_acc, dev_f1, _, _ = evaluate(calibrated_model, dev_loader_cal, DEVICE)[:4]\n",
        "    scheduler_cal.step()\n",
        "\n",
        "    if dev_f1 > best_f1_cal:\n",
        "        best_f1_cal = dev_f1\n",
        "        patience_counter_cal = 0\n",
        "        torch.save(calibrated_model.state_dict(), f'{SAVE_FOLDER}/calibrated_model.pt')\n",
        "    else:\n",
        "        patience_counter_cal += 1\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1:3d} | Loss: {loss:.4f} | Dev F1: {dev_f1:.4f}\")\n",
        "\n",
        "    if patience_counter_cal >= patience_cal:\n",
        "        print(f\"Early stopped at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "calibrated_model.load_state_dict(torch.load(f'{SAVE_FOLDER}/calibrated_model.pt'))\n",
        "test_acc_cal, test_f1_cal, test_prec_cal, test_rec_cal, ece_cal = evaluate(calibrated_model, test_loader_cal, DEVICE, compute_ece=True)\n",
        "\n",
        "print(f\"\\n Calibrated Model: {test_acc_cal*100:.2f}%\")\n",
        "print(f\"   ECE: {ece_cal:.4f}\")\n",
        "print(f\"   Robustness: Trained with 20% modality dropout\\n\")\n",
        "\n",
        "# Final Results\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" Final Summary\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "\n",
        "print(\"  PHASE 1: Working Baseline\")\n",
        "print(f\"{'Model':<30} {'Accuracy':<12} {'F1':<10}\")\n",
        "print(\"-\" * 52)\n",
        "for name, res in baseline_results.items():\n",
        "    print(f\"{name:<30} {res['accuracy']*100:6.2f}%         {res['f1']:.4f}\")\n",
        "\n",
        "print(\"\\n PHASE 2: Stress Test Results\")\n",
        "print(f\"{'Pattern':<30} {'Accuracy':<12} {'Drop':<10}\")\n",
        "print(\"-\" * 52)\n",
        "for pattern, res in missingness_results.items():\n",
        "    print(f\"{pattern:<30} {res['accuracy']*100:6.2f}%         {res['drop']:+.2f}%\")\n",
        "\n",
        "print(\"\\n PHASE 3: Knowledge Distillation\")\n",
        "print(f\"Baseline: {baseline_results['Multimodal_Baseline']['accuracy']*100:.2f}%\")\n",
        "print(f\"KD Student: {test_acc_kd*100:.2f}%\")\n",
        "print(f\"Improvement: {(test_acc_kd - baseline_results['Multimodal_Baseline']['accuracy'])*100:+.2f} pp\")\n",
        "\n",
        "print(\"\\n PHASE 4: Calibration\")\n",
        "print(f\"Accuracy: {test_acc_cal*100:.2f}%\")\n",
        "print(f\"ECE: {ece_cal:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1XL4N8fcs3B"
      },
      "outputs": [],
      "source": [
        "================================================================================\n",
        "ROBUST MULTIMODAL FUSION WITH MISSING MODALITIES\n",
        "\n",
        "================================================================================\n",
        "Device: cuda\n",
        "\n",
        "Loading data...\n",
        "================================================================================\n",
        "PHASE 1: Working Baseline\n",
        "================================================================================\n",
        "\n",
        " Enhanced Architecture:\n",
        "\n",
        "Training TEXT_teacher\n",
        "Epoch  20 | Loss: 0.5578 | Dev F1: 0.6697\n",
        "Early stopped at epoch 33\n",
        " TEXT_teacher: 67.90%\n",
        "\n",
        "Training AUDIO_teacher\n",
        "Epoch  20 | Loss: 0.6595 | Dev F1: 0.5635\n",
        "Epoch  40 | Loss: 0.5496 | Dev F1: 0.5654\n",
        "Early stopped at epoch 48\n",
        " AUDIO_teacher: 58.88%\n",
        "\n",
        "Training VIDEO_teacher\n",
        "Epoch  20 | Loss: 0.6817 | Dev F1: 0.5108\n",
        "Epoch  40 | Loss: 0.6536 | Dev F1: 0.5192\n",
        "Early stopped at epoch 52\n",
        " VIDEO_teacher: 53.83%\n",
        "\n",
        "Training multimodal baseline (with cross-modal attention)\n",
        "Epoch  20 | Loss: 0.5271 | Dev F1: 0.6848\n",
        "Early stopped at epoch 37\n",
        " Multimodal Baseline: 68.48%\n",
        "\n",
        "================================================================================\n",
        "PHASE 2: Stress Test\n",
        "================================================================================\n",
        "\n",
        "Evaluating baseline under missing modalities\n",
        "\n",
        "Missingness Pattern            Accuracy     F1         Drop\n",
        "--------------------------------------------------------------\n",
        "All Present                    0.6848 (68.48%)  0.6844    +0.00%\n",
        "Text Missing                   0.5666 (56.66%)  0.5658    +11.82%\n",
        "Audio Missing                  0.6742 (67.42%)  0.6742    +1.06%\n",
        "Video Missing                  0.6821 (68.21%)  0.6814    +0.27%\n",
        "Text+Audio Missing             0.5161 (51.61%)  0.4649    +16.87%\n",
        "Text+Video Missing             0.5562 (55.62%)  0.5535    +12.86%\n",
        "Audio+Video Missing            0.6696 (66.96%)  0.6696    +1.52%\n",
        "All Missing                    0.5021 (50.21%)  0.3343    +18.27%\n",
        "\n",
        "================================================================================\n",
        "PHASE 3: Cross Modal Knowledge Distillation\n",
        "================================================================================\n",
        "\n",
        "Best teacher: TEXT_teacher (67.90%)\n",
        "\n",
        "Epoch  20 | Loss: 0.5483 | Dev F1: 0.6818\n",
        "Early stopped at epoch 36\n",
        "\n",
        "  KD Student: 69.39% (vs Baseline: 68.48%)\n",
        "   Improvement: +0.91 pp\n",
        "\n",
        "================================================================================\n",
        "PHASE 4: Confidence Calibration under missingness\n",
        "================================================================================\n",
        "\n",
        "Training with 20% modality dropout\n",
        "\n",
        "Epoch  20 | Loss: 0.5222 | Dev F1: 0.6698\n",
        "Early stopped at epoch 34\n",
        "\n",
        "  Calibrated Model: 68.84%\n",
        "   ECE: 0.0577\n",
        "   Robustness: Trained with 20% modality dropout\n",
        "\n",
        "================================================================================\n",
        " Final Summary\n",
        "================================================================================\n",
        "  PHASE 1: Working Baseline\n",
        "Model                          Accuracy     F1\n",
        "----------------------------------------------------\n",
        "TEXT_teacher                    67.90%         0.6781\n",
        "AUDIO_teacher                   58.88%         0.5882\n",
        "VIDEO_teacher                   53.83%         0.5364\n",
        "Multimodal_Baseline             68.48%         0.6844\n",
        "\n",
        " PHASE 2: Stress Test Results\n",
        "Pattern                        Accuracy     Drop\n",
        "----------------------------------------------------\n",
        "All Present                     68.48%         +0.00%\n",
        "Text Missing                    56.66%         +11.82%\n",
        "Audio Missing                   67.42%         +1.06%\n",
        "Video Missing                   68.21%         +0.27%\n",
        "Text+Audio Missing              51.61%         +16.87%\n",
        "Text+Video Missing              55.62%         +12.86%\n",
        "Audio+Video Missing             66.96%         +1.52%\n",
        "All Missing                     50.21%         +18.27%\n",
        "\n",
        " PHASE 3: Knowledge Distillation\n",
        "Baseline: 68.48%\n",
        "KD Student: 69.39%\n",
        "Improvement: +0.91 pp\n",
        "\n",
        " PHASE 4: Calibration\n",
        "Accuracy: 68.84%\n",
        "ECE: 0.0577\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2026-01-09 09:16:07--  https://www.dropbox.com/s/izk6khkrdwcncia/ted_humor_sdk_v1.zip?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/002rz175n5ferwyvif2hv/ted_humor_sdk_v1.zip?rlkey=r8bszra1ez6zedylbx1d4wm99&dl=1 [following]\n",
            "--2026-01-09 09:16:08--  https://www.dropbox.com/scl/fi/002rz175n5ferwyvif2hv/ted_humor_sdk_v1.zip?rlkey=r8bszra1ez6zedylbx1d4wm99&dl=1\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc6315a566efd0962680a98e7777.dl.dropboxusercontent.com/cd/0/inline/C4nVZJXh8dT9E_zEkkvQjZUwO0zl6OvuZw0aQqw3ZevMd5cLmot9X0uLl8D40JG4jOoh1ATHpJi4vvp52RmHZe1MUvJq2oaSC6lx0MypCScvz6Qhvdw4xjYmeTsABipnd7Nvwglq90KcOidn8csENZPa/file?dl=1# [following]\n",
            "--2026-01-09 09:16:08--  https://uc6315a566efd0962680a98e7777.dl.dropboxusercontent.com/cd/0/inline/C4nVZJXh8dT9E_zEkkvQjZUwO0zl6OvuZw0aQqw3ZevMd5cLmot9X0uLl8D40JG4jOoh1ATHpJi4vvp52RmHZe1MUvJq2oaSC6lx0MypCScvz6Qhvdw4xjYmeTsABipnd7Nvwglq90KcOidn8csENZPa/file?dl=1\n",
            "Resolving uc6315a566efd0962680a98e7777.dl.dropboxusercontent.com (uc6315a566efd0962680a98e7777.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to uc6315a566efd0962680a98e7777.dl.dropboxusercontent.com (uc6315a566efd0962680a98e7777.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/C4nc3qzMtcS06GCXpIeur1AHqnnc2VH3hlnSMwSrWqE4zNUIlOZe9645uNhE_RLfi51MbndBavzQyXOMGnj4C1dnAe6dBtdGlZse0KF1NnoQQubsCpwLZfESecAzoDa3QvL8lrOez-FvnirQkHINWc1zpNGIY_9HjaDmFF7gLyzdGIHg05ZQfc1pZkZkJNORhdb1_NwMgi45Zew-AV-uI65TDN7nnKulu75qUIPAdR3kiKZp1h_eGBg56yP8djWfL1jRPubCFiXBrD5Pvsy3cjeAgdofdH2n5W0RNFam2LmrdmCNwwOltAT4gaErgPRD9V_gnpPRaqGzMQOwf4FIeHpTe5VCE1n-c7Z7acL7CxbY4kqeDvlSkWtPDkdIT1tF7l8/file?dl=1 [following]\n",
            "--2026-01-09 09:16:09--  https://uc6315a566efd0962680a98e7777.dl.dropboxusercontent.com/cd/0/inline2/C4nc3qzMtcS06GCXpIeur1AHqnnc2VH3hlnSMwSrWqE4zNUIlOZe9645uNhE_RLfi51MbndBavzQyXOMGnj4C1dnAe6dBtdGlZse0KF1NnoQQubsCpwLZfESecAzoDa3QvL8lrOez-FvnirQkHINWc1zpNGIY_9HjaDmFF7gLyzdGIHg05ZQfc1pZkZkJNORhdb1_NwMgi45Zew-AV-uI65TDN7nnKulu75qUIPAdR3kiKZp1h_eGBg56yP8djWfL1jRPubCFiXBrD5Pvsy3cjeAgdofdH2n5W0RNFam2LmrdmCNwwOltAT4gaErgPRD9V_gnpPRaqGzMQOwf4FIeHpTe5VCE1n-c7Z7acL7CxbY4kqeDvlSkWtPDkdIT1tF7l8/file?dl=1\n",
            "Reusing existing connection to uc6315a566efd0962680a98e7777.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 801445658 (764M) [application/binary]\n",
            "Saving to: ‘ted_humor_sdk_v1.zip?dl=1’\n",
            "\n",
            "       ted_humor_sd   0%[                    ] 432.00K   714KB/s               ^C\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.dropbox.com/s/izk6khkrdwcncia/ted_humor_sdk_v1.zip?dl=1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
