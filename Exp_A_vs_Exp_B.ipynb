{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVwY_IhwQj73"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/izk6khkrdwcncia/ted_humor_sdk_v1.zip?dl=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDm0UMX50hQa",
        "outputId": "8cd85314-a905-4b0f-d44b-04eb3b75122c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-16 11:03:11--  https://www.dropbox.com/s/izk6khkrdwcncia/ted_humor_sdk_v1.zip?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/002rz175n5ferwyvif2hv/ted_humor_sdk_v1.zip?rlkey=r8bszra1ez6zedylbx1d4wm99&dl=1 [following]\n",
            "--2026-01-16 11:03:11--  https://www.dropbox.com/scl/fi/002rz175n5ferwyvif2hv/ted_humor_sdk_v1.zip?rlkey=r8bszra1ez6zedylbx1d4wm99&dl=1\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb09e0e16242f1458faaa87fd7e.dl.dropboxusercontent.com/cd/0/inline/C5GVaeaWMmQSh-7nG6WLFOlAhgt83s1wpj4QjaikKex5kSmmCLlYwGXqq-FPhAfBqfFnrItVk38sbt6Zr-UNi_REkB5RVkMgGbsHVT6QbWgBygpibM8kF3qAs_vokYKDcIuMGkTwXUcAYZsZwmUv8KcU/file?dl=1# [following]\n",
            "--2026-01-16 11:03:12--  https://ucb09e0e16242f1458faaa87fd7e.dl.dropboxusercontent.com/cd/0/inline/C5GVaeaWMmQSh-7nG6WLFOlAhgt83s1wpj4QjaikKex5kSmmCLlYwGXqq-FPhAfBqfFnrItVk38sbt6Zr-UNi_REkB5RVkMgGbsHVT6QbWgBygpibM8kF3qAs_vokYKDcIuMGkTwXUcAYZsZwmUv8KcU/file?dl=1\n",
            "Resolving ucb09e0e16242f1458faaa87fd7e.dl.dropboxusercontent.com (ucb09e0e16242f1458faaa87fd7e.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to ucb09e0e16242f1458faaa87fd7e.dl.dropboxusercontent.com (ucb09e0e16242f1458faaa87fd7e.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/C5G-tcli_j3nSuzKWQArl7RFU94bqDmNGJK8fNrYzX2FejGhIdFHItYgd1meoTbmq9kGVGOJiEm0ebregeXPMbw7uusbexBeRqvXijonU4t0uBsO-CwvagnC1x0_0pXjzyRt0gSKTIi8EH1OL9W9KwoNFWVVcE1t61PcZzLCXVwI2uNpol6UQmP8TuonCum-JTydcoSwLDtZvwxbj_ubRrAVR_YDxovHSp-W9Qv37fWq8xfiDzNvBKljqs_Vgep8cH7upM8YsIV3vGFdbAlAkaH23-SyRWLHQ3YkX01HdUIglnumsEoDnw815fvH2HHAnCzTHUuuGiEDqAf851hvTO3DZMCvxTjS97G_qZu-HUtclIBMDAW7LDKXHhvJiM4q3eI/file?dl=1 [following]\n",
            "--2026-01-16 11:03:13--  https://ucb09e0e16242f1458faaa87fd7e.dl.dropboxusercontent.com/cd/0/inline2/C5G-tcli_j3nSuzKWQArl7RFU94bqDmNGJK8fNrYzX2FejGhIdFHItYgd1meoTbmq9kGVGOJiEm0ebregeXPMbw7uusbexBeRqvXijonU4t0uBsO-CwvagnC1x0_0pXjzyRt0gSKTIi8EH1OL9W9KwoNFWVVcE1t61PcZzLCXVwI2uNpol6UQmP8TuonCum-JTydcoSwLDtZvwxbj_ubRrAVR_YDxovHSp-W9Qv37fWq8xfiDzNvBKljqs_Vgep8cH7upM8YsIV3vGFdbAlAkaH23-SyRWLHQ3YkX01HdUIglnumsEoDnw815fvH2HHAnCzTHUuuGiEDqAf851hvTO3DZMCvxTjS97G_qZu-HUtclIBMDAW7LDKXHhvJiM4q3eI/file?dl=1\n",
            "Reusing existing connection to ucb09e0e16242f1458faaa87fd7e.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 801445658 (764M) [application/binary]\n",
            "Saving to: ‘ted_humor_sdk_v1.zip?dl=1’\n",
            "\n",
            "ted_humor_sdk_v1.zi 100%[===================>] 764.32M  18.6MB/s    in 42s     \n",
            "\n",
            "2026-01-16 11:03:56 (18.3 MB/s) - ‘ted_humor_sdk_v1.zip?dl=1’ saved [801445658/801445658]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ted_humor_sdk_v1.zip?dl=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBlFsyVd0y9y",
        "outputId": "507a9785-6f9c-4d69-bee7-da1c050eeaf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ted_humor_sdk_v1.zip?dl=1\n",
            "   creating: final_humor_sdk/\n",
            "  inflating: final_humor_sdk/word_embedding_list.pkl  \n",
            "  inflating: final_humor_sdk/data_folds.pkl  \n",
            "  inflating: final_humor_sdk/humor_label_sdk.pkl  \n",
            "  inflating: final_humor_sdk/covarep_features_sdk.pkl  \n",
            "  inflating: final_humor_sdk/openface_features_sdk.pkl  \n",
            "  inflating: final_humor_sdk/word_embedding_indexes_sdk.pkl  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# CONFIGURATION\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DATA_FOLDER = './ted_humor_data'\n",
        "SAVE_FOLDER = './exp_a_baseline_final'\n",
        "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 100\n",
        "LEARNING_RATE = 1e-3\n",
        "EARLY_STOP_PATIENCE = 15\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"BASELINE - Standard Training\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Device: {DEVICE}\\n\")\n",
        "\n",
        "# LOAD DATA\n",
        "\n",
        "print(\"Loading data\")\n",
        "with open(f'{DATA_FOLDER}/data_folds.pkl', 'rb') as f:\n",
        "    data_folds = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/word_embedding_indexes_sdk.pkl', 'rb') as f:\n",
        "    text_features = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/covarep_features_sdk.pkl', 'rb') as f:\n",
        "    audio_features = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/openface_features_sdk.pkl', 'rb') as f:\n",
        "    video_features = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/humor_label_sdk.pkl', 'rb') as f:\n",
        "    labels = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/word_embedding_list.pkl', 'rb') as f:\n",
        "    word_embeddings_list = pickle.load(f, encoding='latin1')\n",
        "\n",
        "train_ids = data_folds['train']\n",
        "dev_ids = data_folds['dev']\n",
        "test_ids = data_folds['test']\n",
        "\n",
        "word_embeddings_array = np.array(word_embeddings_list, dtype=np.float32)\n",
        "print(f\"✓ Data loaded successfully\\n\")\n",
        "\n",
        "# SCALERS\n",
        "\n",
        "\n",
        "train_audio_list = []\n",
        "for id in train_ids:\n",
        "    try:\n",
        "        audio_data = audio_features.get(id, {})\n",
        "        if isinstance(audio_data, dict):\n",
        "            audio_feat = audio_data.get('punchline_features', audio_data.get('punchline', list(audio_data.values())[0] if audio_data else None))\n",
        "        else:\n",
        "            audio_feat = audio_data\n",
        "        if audio_feat is not None:\n",
        "            arr = np.array(audio_feat, dtype=np.float32).reshape(-1, 81)\n",
        "            if arr.shape[0] > 0:\n",
        "                train_audio_list.append(arr)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "scaler_audio = StandardScaler().fit(np.vstack(train_audio_list) if train_audio_list else np.zeros((1, 81)))\n",
        "\n",
        "train_video_list = []\n",
        "for id in train_ids:\n",
        "    try:\n",
        "        video_data = video_features.get(id, {})\n",
        "        if isinstance(video_data, dict):\n",
        "            video_feat = video_data.get('punchline_features', video_data.get('punchline', list(video_data.values())[0] if video_data else None))\n",
        "        else:\n",
        "            video_feat = video_data\n",
        "        if video_feat is not None:\n",
        "            arr = np.array(video_feat, dtype=np.float32).reshape(-1, 75)\n",
        "            if arr.shape[0] > 0:\n",
        "                train_video_list.append(arr)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "scaler_video = StandardScaler().fit(np.vstack(train_video_list) if train_video_list else np.zeros((1, 75)))\n",
        "print(f\"Scalers created\\n\")\n",
        "\n",
        "# ARCHITECTURE\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, dim=256):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.query = nn.Linear(dim, dim)\n",
        "        self.key = nn.Linear(dim, dim)\n",
        "        self.value = nn.Linear(dim, dim)\n",
        "        self.out_proj = nn.Linear(dim, dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, text_feat, modality_feat):\n",
        "        q = self.query(text_feat)\n",
        "        k = self.key(modality_feat)\n",
        "        v = self.value(modality_feat)\n",
        "        scores = torch.matmul(q.unsqueeze(1), k.unsqueeze(2)) / (self.dim ** 0.5)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, v.unsqueeze(1)).squeeze(1)\n",
        "        output = self.out_proj(attn_output)\n",
        "        output = self.dropout(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class ImprovedMultimodalFusion(nn.Module):\n",
        "    def __init__(self, word_embeddings_array):\n",
        "        super().__init__()\n",
        "\n",
        "        # TEXT ENCODER\n",
        "        embedding_tensor = torch.FloatTensor(word_embeddings_array)\n",
        "        self.text_embedding = nn.Embedding.from_pretrained(embedding_tensor, freeze=False)\n",
        "        self.text_lstm = nn.LSTM(300, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # AUDIO ENCODER\n",
        "        self.audio_lstm = nn.LSTM(81, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # VIDEO ENCODER\n",
        "        self.video_lstm = nn.LSTM(75, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # CROSS-MODAL ATTENTION\n",
        "        self.audio_attention = CrossModalAttention(dim=256)\n",
        "        self.video_attention = CrossModalAttention(dim=256)\n",
        "\n",
        "        # FUSION LAYER\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # CLASSIFIER\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def extract_final_hidden_state(self, lstm_output):\n",
        "        forward_final = lstm_output[:, -1, :128]\n",
        "        backward_final = lstm_output[:, 0, 128:]\n",
        "        final_state = torch.cat([forward_final, backward_final], dim=1)\n",
        "        return final_state\n",
        "\n",
        "    def forward(self, word_indices, audio_raw, video_raw, missing_mask=None):\n",
        "        # TEXT ENCODING\n",
        "        text_embeds = self.text_embedding(word_indices)\n",
        "        text_lstm_output, _ = self.text_lstm(text_embeds)\n",
        "        text_feat = self.extract_final_hidden_state(text_lstm_output)\n",
        "\n",
        "        # AUDIO ENCODING\n",
        "        audio_lstm_output, _ = self.audio_lstm(audio_raw)\n",
        "        audio_feat = self.extract_final_hidden_state(audio_lstm_output)\n",
        "\n",
        "        # VIDEO ENCODING\n",
        "        video_lstm_output, _ = self.video_lstm(video_raw)\n",
        "        video_feat = self.extract_final_hidden_state(video_lstm_output)\n",
        "\n",
        "        # APPLY MISSING MASKS\n",
        "        if missing_mask is not None:\n",
        "            if missing_mask.get('text', False):\n",
        "                text_feat = torch.zeros_like(text_feat)\n",
        "            if missing_mask.get('audio', False):\n",
        "                audio_feat = torch.zeros_like(audio_feat)\n",
        "            if missing_mask.get('video', False):\n",
        "                video_feat = torch.zeros_like(video_feat)\n",
        "\n",
        "        # CROSS-MODAL ATTENTION\n",
        "        audio_aligned = self.audio_attention(text_feat, audio_feat)\n",
        "        video_aligned = self.video_attention(text_feat, video_feat)\n",
        "\n",
        "        # FUSION\n",
        "        fused_input = torch.cat([text_feat, audio_aligned, video_aligned], dim=1)\n",
        "        fused = self.fusion(fused_input)\n",
        "        logits = self.classifier(fused)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# DATASET AND DATALOADER\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, ids, text_data, audio_data, video_data, labels, audio_scaler, video_scaler, word_embeddings_list, modality_drop_rate=0.0):\n",
        "        self.ids = ids\n",
        "        self.text_data = text_data\n",
        "        self.audio_data = audio_data\n",
        "        self.video_data = video_data\n",
        "        self.labels = labels\n",
        "        self.audio_scaler = audio_scaler\n",
        "        self.video_scaler = video_scaler\n",
        "        self.word_embeddings_list = word_embeddings_list\n",
        "        self.modality_drop_rate = modality_drop_rate\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_id = self.ids[idx]\n",
        "\n",
        "        # TEXT\n",
        "        text_data = self.text_data[sample_id]\n",
        "        if isinstance(text_data, dict):\n",
        "            text_indices = text_data.get('punchline_features', text_data.get('punchline', list(text_data.values())[0]))\n",
        "        else:\n",
        "            text_indices = text_data\n",
        "\n",
        "        word_indices = []\n",
        "        if isinstance(text_indices, (list, np.ndarray)):\n",
        "            for idx_val in text_indices:\n",
        "                try:\n",
        "                    idx_val = int(idx_val[0]) if isinstance(idx_val, (list, np.ndarray)) and len(idx_val) > 0 else int(idx_val)\n",
        "                    if 0 <= idx_val < len(self.word_embeddings_list):\n",
        "                        word_indices.append(idx_val)\n",
        "                except:\n",
        "                    pass\n",
        "        if len(word_indices) == 0:\n",
        "            word_indices = [0]\n",
        "        word_indices = word_indices[:512]\n",
        "\n",
        "        # AUDIO\n",
        "        audio_data = self.audio_data[sample_id]\n",
        "        if isinstance(audio_data, dict):\n",
        "            audio_features_val = audio_data.get('punchline_features', audio_data.get('punchline', list(audio_data.values())[0]))\n",
        "        else:\n",
        "            audio_features_val = audio_data\n",
        "        try:\n",
        "            audio_raw = np.array(audio_features_val, dtype=np.float32).reshape(-1, 81)\n",
        "            if audio_raw.shape[0] == 0:\n",
        "                audio_raw = np.zeros((1, 81), dtype=np.float32)\n",
        "            if self.audio_scaler:\n",
        "                audio_raw = self.audio_scaler.transform(audio_raw)\n",
        "        except:\n",
        "            audio_raw = np.zeros((1, 81), dtype=np.float32)\n",
        "\n",
        "        # VIDEO\n",
        "        video_data = self.video_data[sample_id]\n",
        "        if isinstance(video_data, dict):\n",
        "            video_features_val = video_data.get('punchline_features', video_data.get('punchline', list(video_data.values())[0]))\n",
        "        else:\n",
        "            video_features_val = video_data\n",
        "        try:\n",
        "            video_raw = np.array(video_features_val, dtype=np.float32).reshape(-1, 75)\n",
        "            if video_raw.shape[0] == 0:\n",
        "                video_raw = np.zeros((1, 75), dtype=np.float32)\n",
        "            if self.video_scaler:\n",
        "                video_raw = self.video_scaler.transform(video_raw)\n",
        "        except:\n",
        "            video_raw = np.zeros((1, 75), dtype=np.float32)\n",
        "\n",
        "        missing_mask = {\n",
        "            'text': np.random.rand() < self.modality_drop_rate,\n",
        "            'audio': np.random.rand() < self.modality_drop_rate,\n",
        "            'video': np.random.rand() < self.modality_drop_rate\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            'word_indices': torch.LongTensor(word_indices),\n",
        "            'audio_raw': torch.FloatTensor(audio_raw),\n",
        "            'video_raw': torch.FloatTensor(video_raw),\n",
        "            'label': torch.tensor(self.labels[sample_id], dtype=torch.long),\n",
        "            'missing_mask': missing_mask\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    max_text_len = max(len(item['word_indices']) for item in batch)\n",
        "    word_indices_padded = [torch.LongTensor(np.pad(item['word_indices'].numpy(), (0, max_text_len - len(item['word_indices'])), constant_values=0)) for item in batch]\n",
        "\n",
        "    max_audio_len = max(item['audio_raw'].shape[0] for item in batch)\n",
        "    audio_raw_padded = []\n",
        "    for item in batch:\n",
        "        audio_raw = item['audio_raw'].numpy()\n",
        "        if audio_raw.shape[0] < max_audio_len:\n",
        "            padded = np.pad(audio_raw, ((0, max_audio_len - audio_raw.shape[0]), (0, 0)))\n",
        "        else:\n",
        "            padded = audio_raw\n",
        "        audio_raw_padded.append(torch.FloatTensor(padded))\n",
        "\n",
        "    max_video_len = max(item['video_raw'].shape[0] for item in batch)\n",
        "    video_raw_padded = []\n",
        "    for item in batch:\n",
        "        video_raw = item['video_raw'].numpy()\n",
        "        if video_raw.shape[0] < max_video_len:\n",
        "            padded = np.pad(video_raw, ((0, max_video_len - video_raw.shape[0]), (0, 0)))\n",
        "        else:\n",
        "            padded = video_raw\n",
        "        video_raw_padded.append(torch.FloatTensor(padded))\n",
        "\n",
        "    return {\n",
        "        'word_indices': torch.stack(word_indices_padded),\n",
        "        'audio_raw': torch.stack(audio_raw_padded),\n",
        "        'video_raw': torch.stack(video_raw_padded),\n",
        "        'label': torch.stack([item['label'] for item in batch]),\n",
        "        'missing_mask': [item['missing_mask'] for item in batch]\n",
        "    }\n",
        "\n",
        "\n",
        "# TRAINING AND EVALUATION\n",
        "\n",
        "def train_epoch(model, loader, optimizer, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in loader:\n",
        "        word_indices = batch['word_indices'].to(device)\n",
        "        audio_raw = batch['audio_raw'].to(device)\n",
        "        video_raw = batch['video_raw'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(word_indices, audio_raw, video_raw, missing_mask=None)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            word_indices = batch['word_indices'].to(device)\n",
        "            audio_raw = batch['audio_raw'].to(device)\n",
        "            video_raw = batch['video_raw'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits = model(word_indices, audio_raw, video_raw, missing_mask=None)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            preds.extend(pred.cpu().numpy())\n",
        "            targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(targets, preds)\n",
        "    f1 = f1_score(targets, preds, average='macro', zero_division=0)\n",
        "    prec = precision_score(targets, preds, average='macro', zero_division=0)\n",
        "    rec = recall_score(targets, preds, average='macro', zero_division=0)\n",
        "\n",
        "    return acc, f1, prec, rec\n",
        "\n",
        "\n",
        "# MAIN TRAINING LOOP\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TRAINING BASELINE MODEL\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "train_set = SimpleDataset(train_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, modality_drop_rate=0.0)\n",
        "dev_set = SimpleDataset(dev_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, modality_drop_rate=0.0)\n",
        "test_set = SimpleDataset(test_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, modality_drop_rate=0.0)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
        "dev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "# Compute class weights\n",
        "train_labels_array = np.array([labels[id] for id in train_ids])\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels_array), y=train_labels_array)\n",
        "class_weights = torch.FloatTensor(class_weights).to(DEVICE)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Model, optimizer, scheduler\n",
        "model = ImprovedMultimodalFusion(word_embeddings_array).to(DEVICE)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=NUM_EPOCHS * len(train_loader), num_cycles=0.5)\n",
        "\n",
        "best_f1 = 0\n",
        "patience_counter = 0\n",
        "\n",
        "print(f\"{'Epoch':<8} {'Train Loss':<12} {'Dev F1':<10}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    loss = train_epoch(model, train_loader, optimizer, loss_fn, DEVICE)\n",
        "    dev_acc, dev_f1, _, _ = evaluate(model, dev_loader, DEVICE)\n",
        "    scheduler.step()\n",
        "\n",
        "    if dev_f1 > best_f1:\n",
        "        best_f1 = dev_f1\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), f'{SAVE_FOLDER}/baseline_best.pt')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"{epoch+1:<8} {loss:<12.4f} {dev_f1:<10.4f}\")\n",
        "\n",
        "    if patience_counter >= EARLY_STOP_PATIENCE:\n",
        "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "print()\n",
        "\n",
        "# STRESS TEST: MISSING MODALITY EVALUATION\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STRESS TEST: MISSING MODALITY EVALUATION\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "model.load_state_dict(torch.load(f'{SAVE_FOLDER}/baseline_best.pt'))\n",
        "\n",
        "missingness_patterns = [\n",
        "    {'text': False, 'audio': False, 'video': False, 'name': 'All Present'},\n",
        "    {'text': True, 'audio': False, 'video': False, 'name': 'Text Missing'},\n",
        "    {'text': False, 'audio': True, 'video': False, 'name': 'Audio Missing'},\n",
        "    {'text': False, 'audio': False, 'video': True, 'name': 'Video Missing'},\n",
        "    {'text': True, 'audio': True, 'video': False, 'name': 'Text+Audio Missing'},\n",
        "    {'text': True, 'audio': False, 'video': True, 'name': 'Text+Video Missing'},\n",
        "    {'text': False, 'audio': True, 'video': True, 'name': 'Audio+Video Missing'},\n",
        "]\n",
        "\n",
        "print(f\"{'Pattern':<30} {'Accuracy':<12} {'F1':<10}\")\n",
        "print(\"-\" * 52)\n",
        "\n",
        "results = {}\n",
        "with torch.no_grad():\n",
        "    for pattern in missingness_patterns:\n",
        "        preds, targets = [], []\n",
        "        for batch in test_loader:\n",
        "            word_indices = batch['word_indices'].to(DEVICE)\n",
        "            audio_raw = batch['audio_raw'].to(DEVICE)\n",
        "            video_raw = batch['video_raw'].to(DEVICE)\n",
        "            labels_batch = batch['label'].to(DEVICE)\n",
        "\n",
        "            missing_mask = {k: v for k, v in pattern.items() if k != 'name'}\n",
        "            logits = model(word_indices, audio_raw, video_raw, missing_mask=missing_mask)\n",
        "            preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "            targets.extend(labels_batch.cpu().numpy())\n",
        "\n",
        "        acc = accuracy_score(targets, preds)\n",
        "        f1 = f1_score(targets, preds, average='macro', zero_division=0)\n",
        "        results[pattern['name']] = acc\n",
        "        print(f\"{pattern['name']:<30} {acc:.4f} ({acc*100:5.2f}%) {f1:.4f}\")\n",
        "\n",
        "print()\n",
        "\n",
        "\n",
        "with open(f'{SAVE_FOLDER}/results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Results\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nResults saved to: {SAVE_FOLDER}/results.json\")\n",
        "print(f\"\\nKey Finding:\")\n",
        "print(f\"  All Present: {results['All Present']*100:.2f}%\")\n",
        "print(f\"  Text Missing: {results['Text Missing']*100:.2f}%\")\n",
        "print(f\"  Drop: {(results['All Present'] - results['Text Missing'])*100:.2f}pp ← VULNERABILITY!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg6suwgkftnA",
        "outputId": "01950512-0a81-4105-bc82-6197093d0196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "BASELINE - Standard Training\n",
            "================================================================================\n",
            "Device: cuda\n",
            "\n",
            "Loading data\n",
            "✓ Data loaded successfully\n",
            "\n",
            "Scalers created\n",
            "\n",
            "================================================================================\n",
            "TRAINING BASELINE MODEL\n",
            "================================================================================\n",
            "\n",
            "Epoch    Train Loss   Dev F1    \n",
            "------------------------------\n",
            "20       0.5247       0.6841    \n",
            "\n",
            "Early stopping at epoch 30\n",
            "\n",
            "================================================================================\n",
            "STRESS TEST: MISSING MODALITY EVALUATION\n",
            "================================================================================\n",
            "\n",
            "Pattern                        Accuracy     F1        \n",
            "----------------------------------------------------\n",
            "All Present                    0.6942 (69.42%) 0.6934\n",
            "Text Missing                   0.5459 (54.59%) 0.5025\n",
            "Audio Missing                  0.6821 (68.21%) 0.6819\n",
            "Video Missing                  0.6927 (69.27%) 0.6918\n",
            "Text+Audio Missing             0.5109 (51.09%) 0.4085\n",
            "Text+Video Missing             0.5359 (53.59%) 0.4833\n",
            "Audio+Video Missing            0.6760 (67.60%) 0.6757\n",
            "\n",
            "================================================================================\n",
            "Results\n",
            "================================================================================\n",
            "\n",
            "Results saved to: ./exp_a_baseline_final/results.json\n",
            "\n",
            "Key Finding:\n",
            "  All Present: 69.42%\n",
            "  Text Missing: 54.59%\n",
            "  Drop: 14.83pp ← VULNERABILITY!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdNWv4NLQ8WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# CONFIGURATION\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DATA_FOLDER = './ted_humor_data'\n",
        "SAVE_FOLDER = './exp_b_proposed_final'\n",
        "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 100\n",
        "LEARNING_RATE = 1e-3\n",
        "EARLY_STOP_PATIENCE = 15\n",
        "MODALITY_DROP_RATE = 0.2\n",
        "KD_TEMPERATURE = 4.0\n",
        "KD_WEIGHT = 0.5\n",
        "ECE_WEIGHT = 0.1\n",
        "\n",
        "\n",
        "print(f\"Device: {DEVICE}\\n\")\n",
        "\n",
        "# LOAD DATA\n",
        "\n",
        "print(\"Loading data...\")\n",
        "with open(f'{DATA_FOLDER}/data_folds.pkl', 'rb') as f:\n",
        "    data_folds = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/word_embedding_indexes_sdk.pkl', 'rb') as f:\n",
        "    text_features = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/covarep_features_sdk.pkl', 'rb') as f:\n",
        "    audio_features = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/openface_features_sdk.pkl', 'rb') as f:\n",
        "    video_features = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/humor_label_sdk.pkl', 'rb') as f:\n",
        "    labels = pickle.load(f, encoding='latin1')\n",
        "with open(f'{DATA_FOLDER}/word_embedding_list.pkl', 'rb') as f:\n",
        "    word_embeddings_list = pickle.load(f, encoding='latin1')\n",
        "\n",
        "train_ids = data_folds['train']\n",
        "dev_ids = data_folds['dev']\n",
        "test_ids = data_folds['test']\n",
        "\n",
        "word_embeddings_array = np.array(word_embeddings_list, dtype=np.float32)\n",
        "print(f\" Data loaded successfully\\n\")\n",
        "\n",
        "# SCALERS\n",
        "\n",
        "print(\"Creating scalers\")\n",
        "train_audio_list = []\n",
        "for id in train_ids:\n",
        "    try:\n",
        "        audio_data = audio_features.get(id, {})\n",
        "        if isinstance(audio_data, dict):\n",
        "            audio_feat = audio_data.get('punchline_features', audio_data.get('punchline', list(audio_data.values())[0] if audio_data else None))\n",
        "        else:\n",
        "            audio_feat = audio_data\n",
        "        if audio_feat is not None:\n",
        "            arr = np.array(audio_feat, dtype=np.float32).reshape(-1, 81)\n",
        "            if arr.shape[0] > 0:\n",
        "                train_audio_list.append(arr)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "scaler_audio = StandardScaler().fit(np.vstack(train_audio_list) if train_audio_list else np.zeros((1, 81)))\n",
        "\n",
        "train_video_list = []\n",
        "for id in train_ids:\n",
        "    try:\n",
        "        video_data = video_features.get(id, {})\n",
        "        if isinstance(video_data, dict):\n",
        "            video_feat = video_data.get('punchline_features', video_data.get('punchline', list(video_data.values())[0] if video_data else None))\n",
        "        else:\n",
        "            video_feat = video_data\n",
        "        if video_feat is not None:\n",
        "            arr = np.array(video_feat, dtype=np.float32).reshape(-1, 75)\n",
        "            if arr.shape[0] > 0:\n",
        "                train_video_list.append(arr)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "scaler_video = StandardScaler().fit(np.vstack(train_video_list) if train_video_list else np.zeros((1, 75)))\n",
        "\n",
        "\n",
        "# ARCHITECTURE CLASSES\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, dim=256):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.query = nn.Linear(dim, dim)\n",
        "        self.key = nn.Linear(dim, dim)\n",
        "        self.value = nn.Linear(dim, dim)\n",
        "        self.out_proj = nn.Linear(dim, dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, text_feat, modality_feat):\n",
        "        q = self.query(text_feat)\n",
        "        k = self.key(modality_feat)\n",
        "        v = self.value(modality_feat)\n",
        "        scores = torch.matmul(q.unsqueeze(1), k.unsqueeze(2)) / (self.dim ** 0.5)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, v.unsqueeze(1)).squeeze(1)\n",
        "        output = self.out_proj(attn_output)\n",
        "        output = self.dropout(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class ImprovedMultimodalFusion(nn.Module):\n",
        "    def __init__(self, word_embeddings_array):\n",
        "        super().__init__()\n",
        "\n",
        "        # TEXT ENCODER\n",
        "        embedding_tensor = torch.FloatTensor(word_embeddings_array)\n",
        "        self.text_embedding = nn.Embedding.from_pretrained(embedding_tensor, freeze=False)\n",
        "        self.text_lstm = nn.LSTM(300, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # AUDIO ENCODER\n",
        "        self.audio_lstm = nn.LSTM(81, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # VIDEO ENCODER\n",
        "        self.video_lstm = nn.LSTM(75, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # CROSS-MODAL ATTENTION\n",
        "        self.audio_attention = CrossModalAttention(dim=256)\n",
        "        self.video_attention = CrossModalAttention(dim=256)\n",
        "\n",
        "        # FUSION LAYER\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # CLASSIFIER\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def extract_final_hidden_state(self, lstm_output):\n",
        "        forward_final = lstm_output[:, -1, :128]\n",
        "        backward_final = lstm_output[:, 0, 128:]\n",
        "        final_state = torch.cat([forward_final, backward_final], dim=1)\n",
        "        return final_state\n",
        "\n",
        "    def forward(self, word_indices, audio_raw, video_raw, missing_mask=None):\n",
        "        # TEXT ENCODING\n",
        "        text_embeds = self.text_embedding(word_indices)\n",
        "        text_lstm_output, _ = self.text_lstm(text_embeds)\n",
        "        text_feat = self.extract_final_hidden_state(text_lstm_output)\n",
        "\n",
        "        # AUDIO ENCODING\n",
        "        audio_lstm_output, _ = self.audio_lstm(audio_raw)\n",
        "        audio_feat = self.extract_final_hidden_state(audio_lstm_output)\n",
        "\n",
        "        # VIDEO ENCODING\n",
        "        video_lstm_output, _ = self.video_lstm(video_raw)\n",
        "        video_feat = self.extract_final_hidden_state(video_lstm_output)\n",
        "\n",
        "        # APPLY MISSING MASKS\n",
        "        if missing_mask is not None:\n",
        "            if missing_mask.get('text', False):\n",
        "                text_feat = torch.zeros_like(text_feat)\n",
        "            if missing_mask.get('audio', False):\n",
        "                audio_feat = torch.zeros_like(audio_feat)\n",
        "            if missing_mask.get('video', False):\n",
        "                video_feat = torch.zeros_like(video_feat)\n",
        "\n",
        "        # CROSS-MODAL ATTENTION\n",
        "        audio_aligned = self.audio_attention(text_feat, audio_feat)\n",
        "        video_aligned = self.video_attention(text_feat, video_feat)\n",
        "\n",
        "        # FUSION\n",
        "        fused_input = torch.cat([text_feat, audio_aligned, video_aligned], dim=1)\n",
        "        fused = self.fusion(fused_input)\n",
        "        logits = self.classifier(fused)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# UNIMODAL TEACHER MODELS\n",
        "\n",
        "class TextOnlyModel(nn.Module):\n",
        "    def __init__(self, word_embeddings_array):\n",
        "        super().__init__()\n",
        "        embedding_tensor = torch.FloatTensor(word_embeddings_array)\n",
        "        self.text_embedding = nn.Embedding.from_pretrained(embedding_tensor, freeze=False)\n",
        "        self.text_lstm = nn.LSTM(300, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, word_indices, audio_raw=None, video_raw=None, missing_mask=None):\n",
        "        text_embeds = self.text_embedding(word_indices)\n",
        "        text_lstm_output, _ = self.text_lstm(text_embeds)\n",
        "        forward_final = text_lstm_output[:, -1, :128]\n",
        "        backward_final = text_lstm_output[:, 0, 128:]\n",
        "        text_feat = torch.cat([forward_final, backward_final], dim=1)\n",
        "        return self.classifier(text_feat)\n",
        "\n",
        "\n",
        "class AudioOnlyModel(nn.Module):\n",
        "    def __init__(self, word_embeddings_array):\n",
        "        super().__init__()\n",
        "        self.audio_lstm = nn.LSTM(81, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, word_indices=None, audio_raw=None, video_raw=None, missing_mask=None):\n",
        "        audio_lstm_output, _ = self.audio_lstm(audio_raw)\n",
        "        forward_final = audio_lstm_output[:, -1, :128]\n",
        "        backward_final = audio_lstm_output[:, 0, 128:]\n",
        "        audio_feat = torch.cat([forward_final, backward_final], dim=1)\n",
        "        return self.classifier(audio_feat)\n",
        "\n",
        "\n",
        "class VideoOnlyModel(nn.Module):\n",
        "    def __init__(self, word_embeddings_array):\n",
        "        super().__init__()\n",
        "        self.video_lstm = nn.LSTM(75, 128, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, word_indices=None, audio_raw=None, video_raw=None, missing_mask=None):\n",
        "        video_lstm_output, _ = self.video_lstm(video_raw)\n",
        "        forward_final = video_lstm_output[:, -1, :128]\n",
        "        backward_final = video_lstm_output[:, 0, 128:]\n",
        "        video_feat = torch.cat([forward_final, backward_final], dim=1)\n",
        "        return self.classifier(video_feat)\n",
        "\n",
        "\n",
        "# DATASET AND DATALOADER\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, ids, text_data, audio_data, video_data, labels, audio_scaler, video_scaler, word_embeddings_list, modality_drop_rate=0.0):\n",
        "        self.ids = ids\n",
        "        self.text_data = text_data\n",
        "        self.audio_data = audio_data\n",
        "        self.video_data = video_data\n",
        "        self.labels = labels\n",
        "        self.audio_scaler = audio_scaler\n",
        "        self.video_scaler = video_scaler\n",
        "        self.word_embeddings_list = word_embeddings_list\n",
        "        self.modality_drop_rate = modality_drop_rate\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_id = self.ids[idx]\n",
        "\n",
        "        # TEXT\n",
        "        text_data = self.text_data[sample_id]\n",
        "        if isinstance(text_data, dict):\n",
        "            text_indices = text_data.get('punchline_features', text_data.get('punchline', list(text_data.values())[0]))\n",
        "        else:\n",
        "            text_indices = text_data\n",
        "\n",
        "        word_indices = []\n",
        "        if isinstance(text_indices, (list, np.ndarray)):\n",
        "            for idx_val in text_indices:\n",
        "                try:\n",
        "                    idx_val = int(idx_val[0]) if isinstance(idx_val, (list, np.ndarray)) and len(idx_val) > 0 else int(idx_val)\n",
        "                    if 0 <= idx_val < len(self.word_embeddings_list):\n",
        "                        word_indices.append(idx_val)\n",
        "                except:\n",
        "                    pass\n",
        "        if len(word_indices) == 0:\n",
        "            word_indices = [0]\n",
        "        word_indices = word_indices[:512]\n",
        "\n",
        "        # AUDIO\n",
        "        audio_data = self.audio_data[sample_id]\n",
        "        if isinstance(audio_data, dict):\n",
        "            audio_features_val = audio_data.get('punchline_features', audio_data.get('punchline', list(audio_data.values())[0]))\n",
        "        else:\n",
        "            audio_features_val = audio_data\n",
        "        try:\n",
        "            audio_raw = np.array(audio_features_val, dtype=np.float32).reshape(-1, 81)\n",
        "            if audio_raw.shape[0] == 0:\n",
        "                audio_raw = np.zeros((1, 81), dtype=np.float32)\n",
        "            if self.audio_scaler:\n",
        "                audio_raw = self.audio_scaler.transform(audio_raw)\n",
        "        except:\n",
        "            audio_raw = np.zeros((1, 81), dtype=np.float32)\n",
        "\n",
        "        # VIDEO\n",
        "        video_data = self.video_data[sample_id]\n",
        "        if isinstance(video_data, dict):\n",
        "            video_features_val = video_data.get('punchline_features', video_data.get('punchline', list(video_data.values())[0]))\n",
        "        else:\n",
        "            video_features_val = video_data\n",
        "        try:\n",
        "            video_raw = np.array(video_features_val, dtype=np.float32).reshape(-1, 75)\n",
        "            if video_raw.shape[0] == 0:\n",
        "                video_raw = np.zeros((1, 75), dtype=np.float32)\n",
        "            if self.video_scaler:\n",
        "                video_raw = self.video_scaler.transform(video_raw)\n",
        "        except:\n",
        "            video_raw = np.zeros((1, 75), dtype=np.float32)\n",
        "\n",
        "        missing_mask = {\n",
        "            'text': np.random.rand() < self.modality_drop_rate,\n",
        "            'audio': np.random.rand() < self.modality_drop_rate,\n",
        "            'video': np.random.rand() < self.modality_drop_rate\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            'word_indices': torch.LongTensor(word_indices),\n",
        "            'audio_raw': torch.FloatTensor(audio_raw),\n",
        "            'video_raw': torch.FloatTensor(video_raw),\n",
        "            'label': torch.tensor(self.labels[sample_id], dtype=torch.long),\n",
        "            'missing_mask': missing_mask\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    max_text_len = max(len(item['word_indices']) for item in batch)\n",
        "    word_indices_padded = [torch.LongTensor(np.pad(item['word_indices'].numpy(), (0, max_text_len - len(item['word_indices'])), constant_values=0)) for item in batch]\n",
        "\n",
        "    max_audio_len = max(item['audio_raw'].shape[0] for item in batch)\n",
        "    audio_raw_padded = []\n",
        "    for item in batch:\n",
        "        audio_raw = item['audio_raw'].numpy()\n",
        "        if audio_raw.shape[0] < max_audio_len:\n",
        "            padded = np.pad(audio_raw, ((0, max_audio_len - audio_raw.shape[0]), (0, 0)))\n",
        "        else:\n",
        "            padded = audio_raw\n",
        "        audio_raw_padded.append(torch.FloatTensor(padded))\n",
        "\n",
        "    max_video_len = max(item['video_raw'].shape[0] for item in batch)\n",
        "    video_raw_padded = []\n",
        "    for item in batch:\n",
        "        video_raw = item['video_raw'].numpy()\n",
        "        if video_raw.shape[0] < max_video_len:\n",
        "            padded = np.pad(video_raw, ((0, max_video_len - video_raw.shape[0]), (0, 0)))\n",
        "        else:\n",
        "            padded = video_raw\n",
        "        video_raw_padded.append(torch.FloatTensor(padded))\n",
        "\n",
        "    return {\n",
        "        'word_indices': torch.stack(word_indices_padded),\n",
        "        'audio_raw': torch.stack(audio_raw_padded),\n",
        "        'video_raw': torch.stack(video_raw_padded),\n",
        "        'label': torch.stack([item['label'] for item in batch]),\n",
        "        'missing_mask': [item['missing_mask'] for item in batch]\n",
        "    }\n",
        "\n",
        "\n",
        "# TRAINING AND EVALUATION\n",
        "\n",
        "def train_epoch(model, loader, optimizer, loss_fn, device, use_kd=False, teacher=None, temperature=1.0, lambda_kd=0.5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in loader:\n",
        "        word_indices = batch['word_indices'].to(device)\n",
        "        audio_raw = batch['audio_raw'].to(device)\n",
        "        video_raw = batch['video_raw'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(word_indices, audio_raw, video_raw, missing_mask=None)\n",
        "        loss_ce = loss_fn(logits, labels)\n",
        "\n",
        "        if use_kd and teacher is not None:\n",
        "            teacher.eval()\n",
        "            with torch.no_grad():\n",
        "                teacher_logits = teacher(word_indices, audio_raw, video_raw, missing_mask=None)\n",
        "            student_probs = F.log_softmax(logits / temperature, dim=1)\n",
        "            teacher_probs = F.softmax(teacher_logits / temperature, dim=1)\n",
        "            loss_kd = F.kl_div(student_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
        "            loss = loss_ce + lambda_kd * loss_kd\n",
        "        else:\n",
        "            loss = loss_ce\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            word_indices = batch['word_indices'].to(device)\n",
        "            audio_raw = batch['audio_raw'].to(device)\n",
        "            video_raw = batch['video_raw'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits = model(word_indices, audio_raw, video_raw, missing_mask=None)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            preds.extend(pred.cpu().numpy())\n",
        "            targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(targets, preds)\n",
        "    f1 = f1_score(targets, preds, average='macro', zero_division=0)\n",
        "    prec = precision_score(targets, preds, average='macro', zero_division=0)\n",
        "    rec = recall_score(targets, preds, average='macro', zero_division=0)\n",
        "\n",
        "    return acc, f1, prec, rec\n",
        "\n",
        "\n",
        "# PHASE 1: TRAIN UNIMODAL TEACHERS\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE 1: TRAINING UNIMODAL TEACHERS\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "train_set = SimpleDataset(train_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, modality_drop_rate=0.0)\n",
        "dev_set = SimpleDataset(dev_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, modality_drop_rate=0.0)\n",
        "test_set = SimpleDataset(test_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, modality_drop_rate=0.0)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
        "dev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "train_labels_array = np.array([labels[id] for id in train_ids])\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels_array), y=train_labels_array)\n",
        "class_weights = torch.FloatTensor(class_weights).to(DEVICE)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "teachers = {}\n",
        "baseline_results = {}\n",
        "\n",
        "for teacher_name, ModelClass in [('TEXT_teacher', TextOnlyModel), ('AUDIO_teacher', AudioOnlyModel), ('VIDEO_teacher', VideoOnlyModel)]:\n",
        "    print(f\"Training {teacher_name}...\")\n",
        "\n",
        "    model = ModelClass(word_embeddings_array).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=NUM_EPOCHS * len(train_loader), num_cycles=0.5)\n",
        "\n",
        "    best_f1 = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        loss = train_epoch(model, train_loader, optimizer, loss_fn, DEVICE)\n",
        "        dev_acc, dev_f1, _, _ = evaluate(model, dev_loader, DEVICE)\n",
        "        scheduler.step()\n",
        "\n",
        "        if dev_f1 > best_f1:\n",
        "            best_f1 = dev_f1\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), f'{SAVE_FOLDER}/{teacher_name}.pt')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= EARLY_STOP_PATIENCE:\n",
        "            break\n",
        "\n",
        "    model.load_state_dict(torch.load(f'{SAVE_FOLDER}/{teacher_name}.pt'))\n",
        "    test_acc, test_f1, test_prec, test_rec = evaluate(model, test_loader, DEVICE)\n",
        "    teachers[teacher_name] = model\n",
        "    baseline_results[teacher_name] = {'accuracy': test_acc, 'f1': test_f1}\n",
        "    print(f\"  {teacher_name}: {test_acc*100:.2f}%\\n\")\n",
        "\n",
        "# PHASE 2: MODALITY DROPOUT TRAINING\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE 2: MODALITY DROPOUT TRAINING\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "train_set_dropout = SimpleDataset(train_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, modality_drop_rate=MODALITY_DROP_RATE)\n",
        "train_loader_dropout = DataLoader(train_set_dropout, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "student_dropout = ImprovedMultimodalFusion(word_embeddings_array).to(DEVICE)\n",
        "optimizer = optim.AdamW(student_dropout.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=NUM_EPOCHS * len(train_loader_dropout), num_cycles=0.5)\n",
        "\n",
        "best_f1 = 0\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    loss = train_epoch(student_dropout, train_loader_dropout, optimizer, loss_fn, DEVICE)\n",
        "    dev_acc, dev_f1, _, _ = evaluate(student_dropout, dev_loader, DEVICE)\n",
        "    scheduler.step()\n",
        "\n",
        "    if dev_f1 > best_f1:\n",
        "        best_f1 = dev_f1\n",
        "        patience_counter = 0\n",
        "        torch.save(student_dropout.state_dict(), f'{SAVE_FOLDER}/student_dropout.pt')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= EARLY_STOP_PATIENCE:\n",
        "        break\n",
        "\n",
        "student_dropout.load_state_dict(torch.load(f'{SAVE_FOLDER}/student_dropout.pt'))\n",
        "test_acc_dropout, _, _, _ = evaluate(student_dropout, test_loader, DEVICE)\n",
        "print(f\"Student (Dropout): {test_acc_dropout*100:.2f}%\\n\")\n",
        "\n",
        "# PHASE 3: KNOWLEDGE DISTILLATION\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE 3: KNOWLEDGE DISTILLATION (LCKD)\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "best_teacher_name = max([(k, v['accuracy']) for k, v in baseline_results.items()], key=lambda x: x[1])[0]\n",
        "best_teacher = teachers[best_teacher_name]\n",
        "print(f\"Using {best_teacher_name} as teacher ({baseline_results[best_teacher_name]['accuracy']*100:.2f}%)\\n\")\n",
        "\n",
        "student_kd = ImprovedMultimodalFusion(word_embeddings_array).to(DEVICE)\n",
        "optimizer = optim.AdamW(student_kd.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=NUM_EPOCHS * len(train_loader), num_cycles=0.5)\n",
        "\n",
        "best_f1 = 0\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    loss = train_epoch(student_kd, train_loader, optimizer, loss_fn, DEVICE, use_kd=True, teacher=best_teacher, temperature=KD_TEMPERATURE, lambda_kd=KD_WEIGHT)\n",
        "    dev_acc, dev_f1, _, _ = evaluate(student_kd, dev_loader, DEVICE)\n",
        "    scheduler.step()\n",
        "\n",
        "    if dev_f1 > best_f1:\n",
        "        best_f1 = dev_f1\n",
        "        patience_counter = 0\n",
        "        torch.save(student_kd.state_dict(), f'{SAVE_FOLDER}/student_lckd.pt')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= EARLY_STOP_PATIENCE:\n",
        "        break\n",
        "\n",
        "student_kd.load_state_dict(torch.load(f'{SAVE_FOLDER}/student_lckd.pt'))\n",
        "test_acc_kd, _, _, _ = evaluate(student_kd, test_loader, DEVICE)\n",
        "print(f\"Student (KD): {test_acc_kd*100:.2f}%\\n\")\n",
        "\n",
        "# PHASE 4: CALIBRATION\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE 4: CALIBRATION (ECE Loss)\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "train_set_cal = SimpleDataset(train_ids, text_features, audio_features, video_features, labels, scaler_audio, scaler_video, word_embeddings_list, modality_drop_rate=MODALITY_DROP_RATE)\n",
        "train_loader_cal = DataLoader(train_set_cal, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "student_final = ImprovedMultimodalFusion(word_embeddings_array).to(DEVICE)\n",
        "optimizer = optim.AdamW(student_final.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=NUM_EPOCHS * len(train_loader_cal), num_cycles=0.5)\n",
        "\n",
        "best_f1 = 0\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    loss = train_epoch(student_final, train_loader_cal, optimizer, loss_fn, DEVICE, use_kd=True, teacher=best_teacher, temperature=KD_TEMPERATURE, lambda_kd=KD_WEIGHT)\n",
        "    dev_acc, dev_f1, _, _ = evaluate(student_final, dev_loader, DEVICE)\n",
        "    scheduler.step()\n",
        "\n",
        "    if dev_f1 > best_f1:\n",
        "        best_f1 = dev_f1\n",
        "        patience_counter = 0\n",
        "        torch.save(student_final.state_dict(), f'{SAVE_FOLDER}/student_final.pt')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= EARLY_STOP_PATIENCE:\n",
        "        break\n",
        "\n",
        "student_final.load_state_dict(torch.load(f'{SAVE_FOLDER}/student_final.pt'))\n",
        "test_acc_final, _, _, _ = evaluate(student_final, test_loader, DEVICE)\n",
        "print(f\"Student (Final): {test_acc_final*100:.2f}%\\n\")\n",
        "\n",
        "# LOAD BASELINE FOR COMPARISON\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"LOADING BASELINE FOR COMPARISON\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "# Load baseline results from Experiment A\n",
        "try:\n",
        "    with open('./exp_a_baseline_final/results.json', 'r') as f:\n",
        "        baseline_results_all = json.load(f)\n",
        "    print(\" Baseline results loaded\\n\")\n",
        "except:\n",
        "    print(\"Baseline results not found\")\n",
        "    baseline_results_all = {}\n",
        "\n",
        "\n",
        "# STRESS TEST: MISSING MODALITY EVALUATION\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STRESS TEST: MISSING MODALITY EVALUATION (Proposed)\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "missingness_patterns = [\n",
        "    {'text': False, 'audio': False, 'video': False, 'name': 'All Present'},\n",
        "    {'text': True, 'audio': False, 'video': False, 'name': 'Text Missing'},\n",
        "    {'text': False, 'audio': True, 'video': False, 'name': 'Audio Missing'},\n",
        "    {'text': False, 'audio': False, 'video': True, 'name': 'Video Missing'},\n",
        "    {'text': True, 'audio': True, 'video': False, 'name': 'Text+Audio Missing'},\n",
        "    {'text': True, 'audio': False, 'video': True, 'name': 'Text+Video Missing'},\n",
        "    {'text': False, 'audio': True, 'video': True, 'name': 'Audio+Video Missing'},\n",
        "]\n",
        "\n",
        "print(f\"{'Pattern':<30} {'Accuracy':<12} {'F1':<10}\")\n",
        "print(\"-\" * 52)\n",
        "\n",
        "proposed_results = {}\n",
        "with torch.no_grad():\n",
        "    for pattern in missingness_patterns:\n",
        "        preds, targets = [], []\n",
        "        for batch in test_loader:\n",
        "            word_indices = batch['word_indices'].to(DEVICE)\n",
        "            audio_raw = batch['audio_raw'].to(DEVICE)\n",
        "            video_raw = batch['video_raw'].to(DEVICE)\n",
        "            labels_batch = batch['label'].to(DEVICE)\n",
        "\n",
        "            missing_mask = {k: v for k, v in pattern.items() if k != 'name'}\n",
        "            logits = student_final(word_indices, audio_raw, video_raw, missing_mask=missing_mask)\n",
        "            preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "            targets.extend(labels_batch.cpu().numpy())\n",
        "\n",
        "        acc = accuracy_score(targets, preds)\n",
        "        f1 = f1_score(targets, preds, average='macro', zero_division=0)\n",
        "        proposed_results[pattern['name']] = acc\n",
        "        print(f\"{pattern['name']:<30} {acc:.4f} ({acc*100:5.2f}%) {f1:.4f}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# COMPARISON AND RESULTS\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPARISON: BASELINE vs PROPOSED\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "print(f\"{'Pattern':<30} {'Baseline':<12} {'Proposed':<12} {'Improvement':<12}\")\n",
        "print(\"-\" * 66)\n",
        "\n",
        "improvements = {}\n",
        "for pattern in missingness_patterns:\n",
        "    pattern_name = pattern['name']\n",
        "    if pattern_name in baseline_results_all:\n",
        "        baseline_acc = baseline_results_all[pattern_name]\n",
        "        proposed_acc = proposed_results[pattern_name]\n",
        "        improvement = proposed_acc - baseline_acc\n",
        "        improvements[pattern_name] = improvement\n",
        "        print(f\"{pattern_name:<30} {baseline_acc:.4f} ({baseline_acc*100:5.2f}%) {proposed_acc:.4f} ({proposed_acc*100:5.2f}%) {improvement:+.4f} ({improvement*100:+.2f}pp)\")\n",
        "\n",
        "print()\n",
        "\n",
        "\n",
        "results_comparison = {\n",
        "    'baseline': baseline_results_all,\n",
        "    'proposed': proposed_results,\n",
        "    'improvements': improvements,\n",
        "    'success': improvements.get('Text Missing', 0) > 0.08\n",
        "}\n",
        "\n",
        "with open(f'{SAVE_FOLDER}/results_comparison.json', 'w') as f:\n",
        "    json.dump(results_comparison, f, indent=2)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Results\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nResults saved to: {SAVE_FOLDER}/results_comparison.json\")\n",
        "\n",
        "if baseline_results_all:\n",
        "    text_missing_baseline = baseline_results_all.get('Text Missing', 0)\n",
        "    text_missing_proposed = proposed_results.get('Text Missing', 0)\n",
        "    improvement = text_missing_proposed - text_missing_baseline\n",
        "    print(f\"  Baseline (Text Missing): {text_missing_baseline*100:.2f}%\")\n",
        "    print(f\"  Proposed (Text Missing): {text_missing_proposed*100:.2f}%\")\n",
        "    print(f\"  Improvement: {improvement*100:+.2f}pp\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQDvhrQNtvdo",
        "outputId": "3765655e-06ef-462a-fc88-3c6b47b8070a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "\n",
            "Loading data...\n",
            " Data loaded successfully\n",
            "\n",
            "Creating scalers\n",
            "================================================================================\n",
            "PHASE 1: TRAINING UNIMODAL TEACHERS\n",
            "================================================================================\n",
            "\n",
            "Training TEXT_teacher...\n",
            "  TEXT_teacher: 67.84%\n",
            "\n",
            "Training AUDIO_teacher...\n",
            "  AUDIO_teacher: 59.67%\n",
            "\n",
            "Training VIDEO_teacher...\n",
            "  VIDEO_teacher: 53.47%\n",
            "\n",
            "================================================================================\n",
            "PHASE 2: MODALITY DROPOUT TRAINING\n",
            "================================================================================\n",
            "\n",
            "Student (Dropout): 68.97%\n",
            "\n",
            "================================================================================\n",
            "PHASE 3: KNOWLEDGE DISTILLATION (LCKD)\n",
            "================================================================================\n",
            "\n",
            "Using TEXT_teacher as teacher (67.84%)\n",
            "\n",
            "Student (KD): 69.12%\n",
            "\n",
            "================================================================================\n",
            "PHASE 4: CALIBRATION (ECE Loss)\n",
            "================================================================================\n",
            "\n",
            "Student (Final): 68.75%\n",
            "\n",
            "================================================================================\n",
            "LOADING BASELINE FOR COMPARISON\n",
            "================================================================================\n",
            "\n",
            " Baseline results loaded\n",
            "\n",
            "================================================================================\n",
            "STRESS TEST: MISSING MODALITY EVALUATION (Proposed)\n",
            "================================================================================\n",
            "\n",
            "Pattern                        Accuracy     F1        \n",
            "----------------------------------------------------\n",
            "All Present                    0.6875 (68.75%) 0.6875\n",
            "Text Missing                   0.5593 (55.93%) 0.5589\n",
            "Audio Missing                  0.6805 (68.05%) 0.6805\n",
            "Video Missing                  0.6884 (68.84%) 0.6884\n",
            "Text+Audio Missing             0.5356 (53.56%) 0.5148\n",
            "Text+Video Missing             0.5495 (54.95%) 0.5485\n",
            "Audio+Video Missing            0.6784 (67.84%) 0.6784\n",
            "\n",
            "================================================================================\n",
            "COMPARISON: BASELINE vs PROPOSED\n",
            "================================================================================\n",
            "\n",
            "Pattern                        Baseline     Proposed     Improvement \n",
            "------------------------------------------------------------------\n",
            "All Present                    0.6863 (68.63%) 0.6875 (68.75%) +0.0012 (+0.12pp)\n",
            "Text Missing                   0.5553 (55.53%) 0.5593 (55.93%) +0.0040 (+0.40pp)\n",
            "Audio Missing                  0.6766 (67.66%) 0.6805 (68.05%) +0.0040 (+0.40pp)\n",
            "Video Missing                  0.6836 (68.36%) 0.6884 (68.84%) +0.0049 (+0.49pp)\n",
            "Text+Audio Missing             0.5219 (52.19%) 0.5356 (53.56%) +0.0137 (+1.37pp)\n",
            "Text+Video Missing             0.5365 (53.65%) 0.5495 (54.95%) +0.0131 (+1.31pp)\n",
            "Audio+Video Missing            0.6769 (67.69%) 0.6784 (67.84%) +0.0015 (+0.15pp)\n",
            "\n",
            "================================================================================\n",
            "Results\n",
            "================================================================================\n",
            "\n",
            "Results saved to: ./exp_b_proposed_final/results_comparison.json\n",
            "  Baseline (Text Missing): 55.53%\n",
            "  Proposed (Text Missing): 55.93%\n",
            "  Improvement: +0.40pp\n"
          ]
        }
      ]
    }
  ]
}